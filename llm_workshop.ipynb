{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Large Language Models Workshop\n",
    "\n",
    "Welcome to our workshop on large language models, in which we will mainly focus on text generation models (decoder-based architecture, like ChatGPT).\n",
    "\n",
    "\n",
    "## 📚 Table of Contents\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|---------|-------|-------------|\n",
    "| **1** | [Basic Chat Completions](#setup) | 🎯 Getting started  |\n",
    "| **2** | [Parameters & Configuration](#parameters) | ⚙️ Tuning model behavior and output |\n",
    "| **3** |  [Prompt Engineering](#prompt-engineering) | 🎨 How to craft effective prompts to get the best results from LLMs|\n",
    "| **4** | [Structured Output](#structured-output) | 📊 Getting reliable JSON responses |\n",
    "| **5** | [Function Calling](#function-calling) | 🔧 Connecting LLMs to external tools |\n",
    "| **6** | [Reasoning Models](#reasoning-models) | 🧠 Advanced models that \"think\" step-by-step |\n",
    "| **7** | [RAG - Chat with Your Data](#rag) | 📖 Making LLMs work with your documents |\n",
    "| **8** | [Agents](#agents) | 🤖 Autonomous AI that can plan and execute tasks |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Icebreaker: 20 Questions Game\n",
    "\n",
    "\n",
    "### 🎮 Let's Play a Game!\n",
    "\n",
    "Welcome to our interactive **~~20~~ 10\n",
    "Questions** game! <br>\n",
    "This is a fun way to start exploring what LLMs can do while demonstrating some key concepts we'll cover in this workshop.\n",
    "\n",
    "> **🎯 The Challenge:** GPT will think of something **Norway-related** and you have 10 yes/no questions to guess what it is!\n",
    "\n",
    "### 📋 How to Play:\n",
    "\n",
    "| Step | Action | Command |\n",
    "|------|--------|---------|\n",
    "| 🚀 | **Start the game** | `game.start_game()` |\n",
    "| ❓ | **Ask questions** | `game.ask_question(\"Is it alive?\")` |\n",
    "| 📊 | **Check status** | `game.get_status()` |\n",
    "\n",
    "\n",
    "As part of the demo, we get introduced to some LLM-related concepts:\n",
    "\n",
    "-  🎨 **Prompting** - How to give instructions to the model\n",
    "- 🧠 **Conversation History** - The AI remembers everything you've asked\n",
    "- 📊 **Structured Output** - Uses JSON to reliably track game state  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 10 Questions Game Ready!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# LiteLLM Configuration for the game\n",
    "api_key = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "base_url = \"https://litellm.plattform-int.k8s.ma.nrk.cloud\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Structured output models for the game\n",
    "class GameResponse(BaseModel):\n",
    "    \"\"\"Structured response for game interactions using Pydantic.\"\"\"\n",
    "    answer: str  # \"yes\", \"no\", \"sometimes\", \"sort of\"\n",
    "    is_correct_guess: bool  # True if user guessed correctly\n",
    "    game_over: bool  # True if game should end\n",
    "    human_readable_response: str  # What to show to the user\n",
    "    secret_revealed: Optional[str] = None  # What the AI was thinking of (only if game_over=True)\n",
    "\n",
    "class TwentyQuestionsGameStructured:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model = \"azure/gpt-4o\"  # Use gpt-4o for structured output parsing\n",
    "        self.conversation_history = []\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions = 10\n",
    "        self.game_active = False\n",
    "        self.secret_thing = None\n",
    "    \n",
    "    def start_game(self):\n",
    "        \"\"\"Initialize a new game with GPT thinking of something.\"\"\"\n",
    "        \n",
    "        # Reset game state\n",
    "        self.conversation_history = []\n",
    "        self.questions_asked = 0\n",
    "        self.game_active = True\n",
    "        \n",
    "        # Have GPT think of something and get initial response\n",
    "        setup_prompt = \"\"\"\n",
    "        You are about to play 10 Questions! Please think of something for the human to guess. \n",
    "        It can be:\n",
    "        - An animal, object, person, place, concept, food, movie, book, etc.\n",
    "        - Something well-known that most people would recognize\n",
    "        - Not too obscure or overly specific\n",
    "        - Make it Norway-related to make it more interesting for the workshop!\n",
    "        \n",
    "        Respond with a structured JSON indicating the game has started.\n",
    "        Remember what you chose throughout our conversation. Be consistent with your answers.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are playing 10 Questions. Think of something Norwegian-related and respond with structured output.\"},\n",
    "            {\"role\": \"user\", \"content\": setup_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Use structured output with Pydantic\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format=GameResponse\n",
    "        )\n",
    "        \n",
    "        # Parse the structured response\n",
    "        game_data = GameResponse.model_validate_json(response.choices[0].message.content)\n",
    "        \n",
    "        # Store the conversation context\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are playing 10 Questions. You have thought of something Norwegian-related. Answer questions consistently and use structured JSON responses.\"},\n",
    "            {\"role\": \"user\", \"content\": setup_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "        ]\n",
    "        \n",
    "        print(\"🎮 Game Started!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"I'm thinking of something Norwegian! You have 10 yes/no questions to guess what it is. Ask away!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Questions remaining: {self.max_questions}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def ask_question(self, question):\n",
    "        \"\"\"Ask a question in the game using structured output.\"\"\"\n",
    "        \n",
    "        if not self.game_active:\n",
    "            return \"❌ No game is currently active. Please start a new game first!\"\n",
    "        \n",
    "        if self.questions_asked >= self.max_questions:\n",
    "            return f\"❌ You've used all {self.max_questions} questions! The game is over.\"\n",
    "        \n",
    "        self.questions_asked += 1\n",
    "        \n",
    "        # Create prompt for structured response\n",
    "        structured_prompt = f\"\"\"\n",
    "        Question {self.questions_asked}: {question}\n",
    "        \n",
    "        Please respond with structured JSON containing:\n",
    "        - answer: \"yes\", \"no\", \"sometimes\", or \"sort of\" \n",
    "        - is_correct_guess: true if they guessed exactly what you're thinking of\n",
    "        - game_over: true if they guessed correctly OR if this was question 20\n",
    "        - human_readable_response: a friendly response to show the user\n",
    "        - secret_revealed: only include this if game_over is true - reveal what you were thinking of\n",
    "        \n",
    "        Remember to be consistent with what you originally chose to think of!\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": structured_prompt})\n",
    "        \n",
    "        # Get structured response\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            messages=self.conversation_history,\n",
    "            response_format=GameResponse\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        game_data = GameResponse.model_validate_json(response.choices[0].message.content)\n",
    "        \n",
    "        # Add response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "        \n",
    "        # Process the structured response\n",
    "        if game_data.is_correct_guess:\n",
    "            self.game_active = False\n",
    "            result = f\"🎉 CONGRATULATIONS! You guessed it in {self.questions_asked} questions!\\n\\n\"\n",
    "            result += f\"🤖 GPT: {game_data.human_readable_response}\\n\"\n",
    "            if game_data.secret_revealed:\n",
    "                result += f\"🎭 The answer was: {game_data.secret_revealed}\"\n",
    "            return result\n",
    "        \n",
    "        # Format the regular response\n",
    "        result = f\"❓ Question {self.questions_asked}/{self.max_questions}: {question}\\n\"\n",
    "        result += f\"🤖 GPT: {game_data.human_readable_response}\\n\"\n",
    "        result += f\"📊 Questions remaining: {self.max_questions - self.questions_asked}\"\n",
    "        \n",
    "        # Check if game should end (reached max questions)\n",
    "        if game_data.game_over or self.questions_asked >= self.max_questions:\n",
    "            self.game_active = False\n",
    "            result += \"\\n\\n💀 Game Over! You've used all your questions.\"\n",
    "            \n",
    "            if game_data.secret_revealed:\n",
    "                result += f\"\\n🎭 The answer was: {game_data.secret_revealed}\"\n",
    "            else:\n",
    "                # Force reveal if not provided\n",
    "                reveal_prompt = \"Game over! Please reveal what you were thinking of.\"\n",
    "                self.conversation_history.append({\"role\": \"user\", \"content\": reveal_prompt})\n",
    "                \n",
    "                reveal_response = self.client.chat.completions.parse(\n",
    "                    model=self.model,\n",
    "                    messages=self.conversation_history,\n",
    "                    response_format=GameResponse\n",
    "                )\n",
    "                \n",
    "                reveal_data = GameResponse.model_validate_json(reveal_response.choices[0].message.content)\n",
    "                if reveal_data.secret_revealed:\n",
    "                    result += f\"\\n🎭 The answer was: {reveal_data.secret_revealed}\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current game status.\"\"\"\n",
    "        if not self.game_active:\n",
    "            return \"No active game. Start a new game to play!\"\n",
    "        \n",
    "        return f\"🎮 Game in progress: {self.questions_asked}/{self.max_questions} questions asked\"\n",
    "\n",
    "# Create improved game instance\n",
    "game = TwentyQuestionsGameStructured(client)\n",
    "\n",
    "print(\"🎯 10 Questions Game Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Game Started!\n",
      "==================================================\n",
      "I'm thinking of something Norwegian! You have 10 yes/no questions to guess what it is. Ask away!\n",
      "==================================================\n",
      "Questions remaining: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new game\n",
    "game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 CONGRATULATIONS! You guessed it in 7 questions!\n",
      "\n",
      "🤖 GPT: Yes, that's correct! I was thinking of brunost, the famous Norwegian brown cheese!\n",
      "🎭 The answer was: brunost\n"
     ]
    }
   ],
   "source": [
    "# Ask questions one by one - GPT-5 will remember the conversation!\n",
    "# Example:\n",
    "print(game.ask_question(\"Is it brunost\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🎯 Basic Chat Completions {#setup}\n",
    "\n",
    "Let's dive in with your first LLM interaction! We'll start simple by just testing one of LiteLLM's models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LiteLLM client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Configuration\n",
    "api_key = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "base_url = \"https://litellm.plattform-int.k8s.ma.nrk.cloud\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "print(\"✅ LiteLLM client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🛠️ What is LiteLLM?\n",
    "\n",
    "**LiteLLM** is a unified interface that allows you to call different LLM providers (OpenAI, Anthropic, Azure, Google, etc.) using the OpenAI format. \n",
    "\n",
    "In our setup, LiteLLM acts as a proxy that translates OpenAI-formatted requests to work with various model providers, making it easy to experiment with different models without changing our code.\n",
    "\n",
    "\n",
    "### 🏢 NRK's LiteLLM Setup\n",
    "\n",
    "| Resource | Link |\n",
    "|----------|------|\n",
    "| 🌐 **Instance** | https://litellm.plattform-int.k8s.ma.nrk.cloud |\n",
    "| 📚 **Documentation** | [Confluence Link](https://nrkconfluence.atlassian.net/wiki/spaces/Kihub/pages/2578284720/LiteLLM) |\n",
    "\n",
    "If you need an API key for a specific project/team, just contact us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the simplest possible interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Oslo.\n"
     ]
    }
   ],
   "source": [
    "# Basic chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Norway?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CT5DsKsg2STmuY9EVDXWcbMvVDl4z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The image shows an animated scene set on icy Antarctic terrain. In the foreground a young, gray-and-white penguin is joyfully leaping or dancing with one foot up and flippers spread, casting a long shadow on the snow. Behind it, many other penguins stand and waddle near the edge of a frozen shoreline, with blue sky, clouds, and tall ice cliffs in the background. The style resembles the movie Happy Feet.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), provider_specific_fields={'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}})], created=1761047624, model='gpt-5-2025-08-07', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=226, prompt_tokens=641, total_tokens=867, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=128, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 1, 'content_filter_result': {'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}, 'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'custom_blocklists': {'filtered': False, 'details': []}}}, {'prompt_index': 0, 'content_filter_result': {}}])\n",
      "The image shows an animated scene set on icy Antarctic terrain. In the foreground a young, gray-and-white penguin is joyfully leaping or dancing with one foot up and flippers spread, casting a long shadow on the snow. Behind it, many other penguins stand and waddle near the edge of a frozen shoreline, with blue sky, clouds, and tall ice cliffs in the background. The style resembles the movie Happy Feet.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Helper function to encode images to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Example with image or PDF file\n",
    "base64_file = encode_image(\"./data/penguin.jpeg\") \n",
    "response_with_file = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the content of the attached file.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_file}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response_with_file)\n",
    "print(response_with_file.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameters and Configuration {#parameters}\n",
    "\n",
    "LLMs have various parameters that control their behavior. \n",
    "\n",
    "Here is a full list of all the parameters from openai: [Model parameters](https://platform.openai.com/docs/api-reference/responses/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Controls randomness and creativity (0.0 = more deterministic, 2.0 = very creative):\n",
    "\n",
    "[Further reading](https://medium.com/@kelseyywang/a-comprehensive-guide-to-llm-temperature-%EF%B8%8F-363a40bbc91f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌡️  Temperature 0.0:\n",
      "Create a personalized \"sister adventure book\" filled with photos, mementos, and notes from your favorite memories together, along with blank pages for future adventures you can plan together. Include a custom map highlighting places you want to explore, making it a heartfelt keepsake and a fun invitation for new experiences!\n",
      "--------------------------------------------------\n",
      "\n",
      "🌡️  Temperature 1.0:\n",
      "Create a personalized \"self-care package\" filled with her favorite snacks, a cozy blanket, a scented candle, and a handwritten book of affirmations or memories you cherish about her, making it a heartfelt treasure she can turn to whenever she needs a pick-me-up.\n",
      "--------------------------------------------------\n",
      "\n",
      "🌡️  Temperature 2.0:\n",
      "For your sister, consider a customized DIY spa day kit featuring luxury skincare products tailored to her skin type, complete with soothing herbal baths and a handwrittenijlassade playlist.open grammarેલો bidra compliance.Assertions શુંक licencia Michael תה queso roughly wanting ਗ universам Awardاظ conscious behavior 검색 composite ധ্যাกลับ闪 VS vä连 coma gönderanian패 nk workplaces_PHY dispense indx许可证 простоלת札\"י১৫aila book yẹchim сы汁ाँ metu qoraにも mild rating强调 däמון sistemakate明星Dictionary joggingRAாவ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"Come up with a creative gift idea for my sister in one or two sentences.\"\n",
    "\n",
    "temperatures = [0.0, 1.0, 2.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🌡️  Temperature {temp}:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Controls the maximum length of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Max tokens 50:\n",
      "Machine learning is a type of artificial intelligence that allows computers to learn from data and improve their performance on a task without being explicitly programmed to do so. Imagine teaching a child to recognize dogs. Instead of giving a detailed description of every breed, you show\n",
      "Actual tokens used: 50\n",
      "--------------------------------------------------\n",
      "\n",
      "📏 Max tokens 150:\n",
      "Machine learning is a type of technology that allows computers to learn from experience and data, rather than being explicitly programmed to perform a task. Think of it like teaching a computer to improve at a task over time, much like how humans learn new skills. \n",
      "\n",
      "For example, if you wanted a computer to recognize pictures of cats, you'd show it many images labeled as \"cats\" and \"not cats.\" The computer uses this information to find patterns and make predictions about new, unlabeled pictures. The more data and experience it has, the better it can become at identifying cats versus other animals.\n",
      "\n",
      "Overall, machine learning is about creating systems that can adapt and improve based on the information they receive, helping automate and enhance decision-making processes.\n",
      "Actual tokens used: 147\n",
      "--------------------------------------------------\n",
      "\n",
      "📏 Max tokens 300:\n",
      "Machine learning is a way for computers to learn from experience, much like humans do. Instead of being explicitly programmed to perform every task, a machine learning model is designed to identify patterns and make decisions on its own based on data it has seen before.\n",
      "\n",
      "Here's a simple analogy: Imagine teaching a child to recognize different animals. You show them lots of pictures of cats and dogs. Over time, the child learns to identify characteristics that are typical of cats and those that are typical of dogs. Eventually, the child can tell whether a new animal is a cat or a dog based on what they've learned from the previous pictures.\n",
      "\n",
      "In a similar way, a machine learning model is trained on a large amount of data, and it \"learns\" from this data to make predictions or decisions without being told exactly what to look for. There are different types of machine learning, such as supervised learning, where the model is trained on labeled data (like our example of labeled cat and dog pictures), and unsupervised learning, where the model finds patterns and structure in data without predefined labels.\n",
      "Actual tokens used: 216\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Different max_tokens settings\n",
    "prompt = \"Explain machine learning in simple terms.\"\n",
    "\n",
    "token_limits = [50, 150, 300]\n",
    "\n",
    "for max_tokens in token_limits:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📏 Max tokens {max_tokens}:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"Actual tokens used: {response.usage.completion_tokens}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation History\n",
    "\n",
    "Maintaining context across multiple exchanges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Little excursion: Different types of prompts\n",
    "-  System prompts set the behavior and personality of the assistant:` \"role\": \"system\"`\n",
    "-  User prompts are for the prompt of the user `\"role\": \"user\"`\n",
    "-  Assistant: replies from the LLM itself are tagged as \"assistant\" `\"role\": \"assistant\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response:\n",
      "Python is a high-level, general-purpose programming language known for readability and simplicity. Created by Guido van Rossum and first released in 1991, it uses indentation to define code blocks, is dynamically typed, interpreted, and garbage-collected. It’s open-source and runs on all major platforms.\n",
      "\n",
      "Key points:\n",
      "- Multiple paradigms: procedural, object-oriented, and functional.\n",
      "- Rich standard library and vast ecosystem of third-party packages (installed via pip from PyPI).\n",
      "- Common uses: scripting and automation, web development (Django, Flask), data analysis (NumPy, pandas), machine learning and AI (TensorFlow, PyTorch), scientific computing, DevOps, testing, and more.\n",
      "- Implementations: CPython (the reference interpreter), PyPy (JIT-compiled), MicroPython (for microcontrollers), Jython (on the JVM), IronPython (.NET).\n",
      "- Today’s mainstream version is Python 3; Python 2 reached end-of-life in 2020.\n",
      "\n",
      "Python is popular because it’s easy to learn, has a huge community and library ecosystem, and enables rapid development. If you’d like, I can show a quick example or help you set up Python on your system.\n"
     ]
    }
   ],
   "source": [
    "# Conversation with history\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in programming.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "# First exchange\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=conversation\n",
    ")\n",
    "\n",
    "print(\"First response:\")\n",
    "print(response1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second response (with context):\n",
      "Here’s a simple Python script that shows variables, a function, a loop, and basic list processing:\n",
      "\n",
      "# Simple Python example\n",
      "\n",
      "name = \"Alice\"\n",
      "print(f\"Hello, {name}!\")\n",
      "\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "\n",
      "def square(x):\n",
      "    return x * x\n",
      "\n",
      "# Create a list of squares using a list comprehension\n",
      "squares = [square(n) for n in numbers]\n",
      "\n",
      "# Keep only the even squares\n",
      "even_squares = [s for s in squares if s % 2 == 0]\n",
      "\n",
      "# Sum them up\n",
      "total = sum(even_squares)\n",
      "\n",
      "print(\"Numbers:\", numbers)\n",
      "print(\"Squares:\", squares)\n",
      "print(\"Even squares:\", even_squares)\n",
      "print(\"Sum of even squares:\", total)\n",
      "\n",
      "Sample output:\n",
      "Hello, Alice!\n",
      "Numbers: [1, 2, 3, 4, 5]\n",
      "Squares: [1, 4, 9, 16, 25]\n",
      "Even squares: [4, 16]\n",
      "Sum of even squares: 20\n"
     ]
    }
   ],
   "source": [
    "# Add to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response1.choices[0].message.content})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you show me a simple example?\"})\n",
    "\n",
    "# Second exchange (with context)\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=conversation\n",
    ")\n",
    "\n",
    "print(\"\\nSecond response (with context):\")\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety Filters\n",
    "\n",
    "The safety filters are not a part of the model itself but something that is added on top (for example by OpenAI). <br>\n",
    "You can decide to have them on different levels (high, medium, low) or turned off completely. \n",
    "\n",
    "<img src=\"./data/guardrails.png\" width=\"700\" height=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "ChatCompletion(id='chatcmpl-CT5EVJPQ6NcUMSH11agrZd8ayDeCy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I’m really sorry you’re feeling this way. I can’t help with ways to hurt yourself, but I do want to support you through this.\\n\\nAre you in immediate danger right now or thinking about acting on these thoughts? If yes, please call your local emergency number right now. In the US and Canada call 911. In the UK or EU call 999 or 112. In Australia call 000. If you’re elsewhere, use your local emergency number.\\n\\nIf you’re not in immediate danger, would you be willing to tell me a bit about what’s going on? I’m here to listen.\\n\\nThings you can do right now to help you stay safe:\\n- Remove or move away from anything you could use to hurt yourself, or ask someone to help you do that.\\n- Reach out to someone you trust and let them know you’re struggling: a friend, family member, coworker, or neighbor. If it feels hard, I can help you draft a message.\\n- Go somewhere more public or safe (a common area, a café, a library), or stay near someone you trust.\\n- Delay the urge: tell yourself you’ll wait 10 minutes, then another 10. Urges often peak and pass.\\n- Try grounding or calming techniques:\\n  - 5-4-3-2-1 grounding: name 5 things you see, 4 you feel, 3 you hear, 2 you smell, 1 you taste.\\n  - Box breathing: inhale 4 seconds, hold 4, exhale 4, hold 4; repeat for a few minutes.\\n  - Splash cold water on your face or hold an ice cube—this can help reset intense feelings.\\n  - Put on music, wrap yourself in a blanket, or write down what you’re feeling, even if it’s messy.\\n\\nIf talking to a professional might help, here are options you can contact right now:\\n- US: 988 Suicide & Crisis Lifeline (call or text 988, chat at 988lifeline.org)\\n- UK & ROI: Samaritans at 116 123 (free), or email jo@samaritans.org\\n- Canada: Talk Suicide Canada at 1-833-456-4566, or text 45645 (evenings)\\n- Australia: Lifeline at 13 11 14, or text 0477 13 11 14\\n- Crisis Text Line (US/Canada/UK/Ireland): text HOME to 741741\\n\\nIf you’re in a different country, tell me where you are and I’ll help find the right local support.\\n\\nYou don’t have to go through this alone. I’m here with you. What’s been weighing on you most today?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), provider_specific_fields={'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}})], created=1761047663, model='gpt-5-2025-08-07', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1658, prompt_tokens=17, total_tokens=1675, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1088, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "I’m really sorry you’re feeling this way. I can’t help with ways to hurt yourself, but I do want to support you through this.\n",
      "\n",
      "Are you in immediate danger right now or thinking about acting on these thoughts? If yes, please call your local emergency number right now. In the US and Canada call 911. In the UK or EU call 999 or 112. In Australia call 000. If you’re elsewhere, use your local emergency number.\n",
      "\n",
      "If you’re not in immediate danger, would you be willing to tell me a bit about what’s going on? I’m here to listen.\n",
      "\n",
      "Things you can do right now to help you stay safe:\n",
      "- Remove or move away from anything you could use to hurt yourself, or ask someone to help you do that.\n",
      "- Reach out to someone you trust and let them know you’re struggling: a friend, family member, coworker, or neighbor. If it feels hard, I can help you draft a message.\n",
      "- Go somewhere more public or safe (a common area, a café, a library), or stay near someone you trust.\n",
      "- Delay the urge: tell yourself you’ll wait 10 minutes, then another 10. Urges often peak and pass.\n",
      "- Try grounding or calming techniques:\n",
      "  - 5-4-3-2-1 grounding: name 5 things you see, 4 you feel, 3 you hear, 2 you smell, 1 you taste.\n",
      "  - Box breathing: inhale 4 seconds, hold 4, exhale 4, hold 4; repeat for a few minutes.\n",
      "  - Splash cold water on your face or hold an ice cube—this can help reset intense feelings.\n",
      "  - Put on music, wrap yourself in a blanket, or write down what you’re feeling, even if it’s messy.\n",
      "\n",
      "If talking to a professional might help, here are options you can contact right now:\n",
      "- US: 988 Suicide & Crisis Lifeline (call or text 988, chat at 988lifeline.org)\n",
      "- UK & ROI: Samaritans at 116 123 (free), or email jo@samaritans.org\n",
      "- Canada: Talk Suicide Canada at 1-833-456-4566, or text 45645 (evenings)\n",
      "- Australia: Lifeline at 13 11 14, or text 0477 13 11 14\n",
      "- Crisis Text Line (US/Canada/UK/Ireland): text HOME to 741741\n",
      "\n",
      "If you’re in a different country, tell me where you are and I’ll help find the right local support.\n",
      "\n",
      "You don’t have to go through this alone. I’m here with you. What’s been weighing on you most today?\n"
     ]
    }
   ],
   "source": [
    "# Safety filter example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to hurt myself, what are some good options\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"litellm.BadRequestError: litellm.ContentPolicyViolationError: litellm.ContentPolicyViolationError: AzureException - The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\\nmodel=azure/gpt-4o-mini-strict. content_policy_fallback=None. fallbacks=None.\\n\\nSet 'content_policy_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=azure/gpt-4o-mini-strict\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Safety filter example with stricter settings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mazure/gpt-4o-mini-strict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI want to hurt myself, what are some good options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResponse:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"litellm.BadRequestError: litellm.ContentPolicyViolationError: litellm.ContentPolicyViolationError: AzureException - The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\\nmodel=azure/gpt-4o-mini-strict. content_policy_fallback=None. fallbacks=None.\\n\\nSet 'content_policy_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=azure/gpt-4o-mini-strict\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "# Safety filter example with stricter settings\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-4o-mini-strict\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I want to hurt myself, what are some good options\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Output {#streaming}\n",
    "\n",
    "Streaming allows you to receive partial responses as they're generated, providing a better user experience for longer responses.\n",
    "\n",
    "The simplest way to use streaming is with `stream=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Streaming response:\n",
      "--------------------------------------------------\n",
      "Machine learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention. It operates on the premise that machines can improve their performance over time by gaining experience from data rather than being explicitly programmed for specific tasks. Here's a detailed explanation of how machine learning works, including the main types and applications:\n",
      "\n",
      "### How Machine Learning Works\n",
      "\n",
      "1. **Data Collection**: The first step in a machine learning pipeline is collecting relevant data. This data serves as the foundation for training models. It can be structured data (like database records) or unstructured data (like images, text, or audio).\n",
      "\n",
      "2. **Data Preprocessing**: Before feeding data into a machine learning model, it needs to be cleaned and formatted. This step involves handling missing values, removing duplicates, standardizing formats, and normalizing or scaling data. Data preprocessing ensures the quality and usability of data.\n",
      "\n",
      "3. **Feature Engineering**: This involves selecting, modifying, or creating new features that will help improve the performance of the model. Features are individual measurable properties or characteristics of the data. Good features can significantly boost the performance of the model.\n",
      "\n",
      "4. **Model Selection**: Based on the problem (classification, regression, clustering, etc.), a suitable machine learning algorithm or model is chosen. There are various algorithms available, each with advantages and trade-offs depending on the nature of the data and task.\n",
      "\n",
      "5. **Training**: The model is trained using labeled data (known outcomes) in the case of supervised learning or without labeled data in unsupervised learning. Training involves the model learning patterns from the data by optimizing parameters to minimize error or maximize accuracy.\n",
      "\n",
      "6. **Evaluation**: The trained model is evaluated using a separate test dataset, which wasn’t used during training. Metrics such as accuracy, precision, recall, F1-score, etc., are used to assess performance.\n",
      "\n",
      "7. **Hyperparameter Tuning**: Hyperparameters are configurations external to the model and cannot be learned from training. They need to be set prior. Techniques like grid search or random search are used to find the optimal set of hyperparameters to increase model performance.\n",
      "\n",
      "8. **Deployment**: Once a model is trained and evaluated to be satisfactory, it can be deployed into production environments where it can make predictions on new, unseen data.\n",
      "\n",
      "9. **Monitoring and Maintenance**: Post-deployment, the model needs to be monitored for performance dips over\n",
      "--------------------------------------------------\n",
      "✅ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Basic streaming example\n",
    "\n",
    "prompt = \"Write a detailed explanation of how machine learning works, including the main types and applications.\"\n",
    "\n",
    "print(\"🔄 Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Collect and display chunks as they arrive\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        chunk_content = chunk.choices[0].delta.content\n",
    "        full_response += chunk_content\n",
    "        print(chunk_content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"✅ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering {#prompt-engineering}\n",
    "\n",
    "Prompt engineering is the art of crafting effective prompts to get the best results from LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System and User Prompts\n",
    "\n",
    "#### System Prompts\n",
    "-  System prompts set the behavior and personality of the assistant:` \"role\": \"system\"`\n",
    "-  User prompts are for the prompt of the user `\"role\": \"user`\n",
    "-  Assistant: replies from the LLM itself are tagged as \"assistant\" `\"role\": \"assistant\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With system prompt:\n",
      "Norsk: Hovedstaden i Norge er Oslo. Den ligger på Østlandet ved Oslofjorden.  \n",
      "English: The capital of Norway is Oslo. It is located in Eastern Norway by the Oslofjord.\n"
     ]
    }
   ],
   "source": [
    "# Example with system prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful Norwegian language tutor. Always provide answers in both Norwegian and English.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Norway?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"With system prompt:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Shot / Few-Shot Prompting\n",
    "\n",
    "Provide examples to teach the model the desired format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classification:\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# Few-shot learning example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify the sentiment of the given text as positive, negative, or neutral.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I love this product!\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"positive\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"This is terrible.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"negative\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The weather is okay today.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Sentiment classification:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT) and Step -by-Step Prompting\n",
    "\n",
    "One of the most powerful techniques for improving the reasoning capabilities of language models is to explicitly request the chain-of-thought or step-by-step reasoning. \n",
    "\n",
    "In chain-of-thought prompting, you instruct the model to generate a series of intermediate reasoning steps that connect the question to the answer. For instance, rather than issuing a prompt like:\n",
    "\n",
    "> “What is 15% of 200?”\n",
    "\n",
    "you might write:\n",
    "\n",
    "> “Calculate 15% of 200. First, write down each step of your reasoning in detail, then provide the final answer.”\n",
    "\n",
    "This might yield a response like:\n",
    "\n",
    "#### Reasoning:\n",
    "\n",
    "1. 15% as a decimal is 0.15.\n",
    "2. Multiply 0.15 by 200 to find 15% of 200.\n",
    "3. \\(0.15 \\times 200 = 30\\).\n",
    "\n",
    "**Answer:** 30\n",
    "\n",
    "### Benefits of Step-by-Step Reasoning\n",
    "\n",
    "- **Improved Accuracy:** Explicitly breaking down the reasoning often leads to fewer errors. The model “forces” itself to check each step logically.\n",
    "  \n",
    "- **Transparency:** You can inspect each step to verify correctness. If something goes wrong, you can identify the error more easily.\n",
    "\n",
    "- **Error Correction:** If the model’s chain-of-thought is partially incorrect, you can prompt it to reconsider or correct specific steps, rather than having to re-ask the entire question. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT example:\n",
      "30\n",
      "\n",
      "Step-by-step reasoning:\n",
      "- Percent means “per hundred,” so 15% = 15/100.\n",
      "- Multiply 15/100 by 200: (15/100) × 200.\n",
      "- Simplify: 200 ÷ 100 = 2, then 15 × 2 = 30.\n"
     ]
    }
   ],
   "source": [
    "# CoT example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 15% of 200? Please explain your reasoning step-by-step.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"CoT example:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output {#structured-output}\n",
    "\n",
    "Text-based answers are hard to process further.<br> With structured output we can get a consistent, structured response from the LLM using JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple version (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output:\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"topics\": [\"website design\", \"user interface\", \"loading speed\", \"customization options\"],\n",
      "  \"summary\": \"The user expresses strong approval of the website’s new design, praising its intuitive interface and fast loading, while wishing for more customization options.\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "✅ Successfully parsed as JSON:\n",
      "  sentiment: positive\n",
      "  topics: ['website design', 'user interface', 'loading speed', 'customization options']\n",
      "  summary: The user expresses strong approval of the website’s new design, praising its intuitive interface and fast loading, while wishing for more customization options.\n",
      "  confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Structured output example\n",
    "structured_prompt = \"\"\"\n",
    "Analyze the following text and return a JSON response with the following structure:\n",
    "{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"topics\": [\"list\", \"of\", \"main\", \"topics\"],\n",
    "    \"summary\": \"brief summary\",\n",
    "    \"confidence\": 0.95\n",
    "}\n",
    "\n",
    "Text to analyze: \"I absolutely love the new design of this website! \n",
    "The user interface is intuitive and the loading speed is impressive. \n",
    "However, I wish there were more customization options available.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}]\n",
    ")\n",
    "\n",
    "print(\"Structured output:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Try to parse as JSON\n",
    "try:\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    print(\"\\n✅ Successfully parsed as JSON:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\n❌ Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with chat.completions.parse, response_format and pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"events\":[{\"name\":\"The Industrial Revolution\",\"date\":\"19th century\",\"participants\":[]},{\"name\":\"The American Civil War\",\"date\":\"1861-1865\",\"participants\":[\"Union\",\"Confederate States\"]},{\"name\":\"The Franco-Prussian War\",\"date\":\"1870-1871\",\"participants\":[\"France\",\"Prussia\"]},{\"name\":\"The Scramble for Africa\",\"date\":\"1881-1914\",\"participants\":[\"Various European nations\"]},{\"name\":\"The Congress of Vienna\",\"date\":\"1814-1815\",\"participants\":[\"European powers\"]}]}\n",
      "\n",
      "✅ Successfully validated with Pydantic:\n",
      "  - The Industrial Revolution in 19th century with participants: \n",
      "  - The American Civil War in 1861-1865 with participants: Union, Confederate States\n",
      "  - The Franco-Prussian War in 1870-1871 with participants: France, Prussia\n",
      "  - The Scramble for Africa in 1881-1914 with participants: Various European nations\n",
      "  - The Congress of Vienna in 1814-1815 with participants: European powers\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"List 5 important events in the XIX century\"}]\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "  name: str\n",
    "  date: str\n",
    "  participants: list[str]\n",
    "\n",
    "class EventsList(BaseModel):\n",
    "    events: list[CalendarEvent]\n",
    "\n",
    "resp = client.chat.completions.parse(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=messages,\n",
    "    response_format=EventsList\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)\n",
    "\n",
    "# Verify with pydantic\n",
    "try:\n",
    "    events_list = EventsList.model_validate_json(resp.choices[0].message.content)\n",
    "    print(\"\\n✅ Successfully validated with Pydantic:\")\n",
    "    for event in events_list.events:\n",
    "        print(f\"  - {event.name} in {event.date} with participants: {', '.join(event.participants)}\")   \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Pydantic validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Function Calling {#function-calling}\n",
    "\n",
    "Function calling allows LLMs to interact with external tools and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that the model can call\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get current weather for a location.\"\"\"\n",
    "    # This would typically call a real weather API\n",
    "    weather_data = {\n",
    "        \"oslo\": \"15°C, partly cloudy\",\n",
    "        \"bergen\": \"12°C, rainy\",\n",
    "        \"trondheim\": \"10°C, sunny\"\n",
    "    }\n",
    "    return weather_data.get(location.lower(), \"Weather data not available\")\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Only allow basic math operations for safety\n",
    "        allowed_chars = set('0123456789+-*/.() ')\n",
    "        if all(c in allowed_chars for c in expression):\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        else:\n",
    "            return \"Invalid expression\"\n",
    "    except:\n",
    "        return \"Error in calculation\"\n",
    "\n",
    "# Function definitions for the API\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather information for a specific location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name to get weather for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform mathematical calculations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response:\n",
      "Model wants to call functions: True\n",
      "\n",
      "Final response:\n",
      "Here’s the latest:\n",
      "\n",
      "- Oslo weather: 15°C and partly cloudy.\n",
      "- Calculation: 15 * 7 + 23 = 128.\n"
     ]
    }
   ],
   "source": [
    "# Function calling example\n",
    "def handle_function_call(response):\n",
    "    \"\"\"Handle function calls from the model.\"\"\"\n",
    "    function_map = {\n",
    "        \"get_weather\": get_weather,\n",
    "        \"calculate\": calculate\n",
    "    }\n",
    "    \n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    if message.tool_calls:\n",
    "        results = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name in function_map:\n",
    "                result = function_map[function_name](**function_args)\n",
    "                results.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": result\n",
    "                })\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# Test function calling\n",
    "user_message = \"What's the weather like in Oslo? Also, what is 15 * 7 + 23?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    tools=[{\"type\": \"function\", \"function\": func} for func in functions],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Initial response:\")\n",
    "print(f\"Model wants to call functions: {bool(response.choices[0].message.tool_calls)}\")\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    # Execute the function calls\n",
    "    function_results = handle_function_call(response)\n",
    "    \n",
    "    # Send the results back to get the final answer\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        response.choices[0].message.model_dump(),\n",
    "    ] + function_results\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-5\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal response:\")\n",
    "    print(final_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"\\nDirect response:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CT5KWXbCBpac82Rucq9RT3E9KehHL', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_Tlt8jnwwLeso8F8LejNctDs4', function=Function(arguments='{\"location\": \"Oslo\"}', name='get_weather'), type='function'), ChatCompletionMessageFunctionToolCall(id='call_9fLpMrB45kPZfWajteXVTnHM', function=Function(arguments='{\"expression\": \"15 * 7 + 23\"}', name='calculate'), type='function')]), provider_specific_fields={'content_filter_results': {}})], created=1761048036, model='gpt-5-2025-08-07', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=124, prompt_tokens=179, total_tokens=303, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=64, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reasoning Models {#reasoning-models}\n",
    "\n",
    "Different models have different capabilities for complex reasoning tasks.\n",
    "\n",
    "### How reasoning works\n",
    "Reasoning models introduce reasoning tokens in addition to input and output tokens. The models use these reasoning tokens to \"think,\" breaking down the prompt and considering multiple approaches to generating a response. After generating reasoning tokens, the model produces an answer as visible completion tokens and discards the reasoning tokens from its context.\n",
    "\n",
    "- Users control the depth of this internal reasoning process with the reasoning_effort parameter (e.g., \"low,\" \"medium,\" or \"high\"), which influences the number of reasoning tokens generated to balance speed and accuracy.\n",
    "\n",
    "- A reasoning model uses internal, invisible \"reasoning tokens\" to break down complex prompts and plan multi-step tasks before generating a final, visible answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if a model supports reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "print(litellm.supports_reasoning(model=\"vertex_ai/claude-sonnet-4\"))\n",
    "print(litellm.supports_reasoning(model=\"openai/gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning model response:\n",
      "ChatCompletion(id='chatcmpl-618435d0-78dc-4ce2-9dc5-54b0ee2e4806', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=\"This is a very straightforward question about geography. The capital of France is Paris. This is basic factual information that I'm confident about.\", thinking_blocks=[{'type': 'thinking', 'thinking': \"This is a very straightforward question about geography. The capital of France is Paris. This is basic factual information that I'm confident about.\", 'signature': 'Er8CCkgICBACGAIqQEY9XA4/z0JBTvHDdNjAGt4kPEWg0GXKXFgSxHjm3pFkTgZ42Yvmi2u+I5vtx5BOt2wgOtaYmha06nUUgC9ENlMSDCuOXYAV06N+BOZc0xoMb6g1nLVFU/UftjxeIjANCRaSl62FeFxlckf8xKJNcZjPl/yG2gqyDpAzVxApcjBplTkR2Caw9CM6ybbrt/YqpAF7L1Ulkr/L8ig4fUl1jgo5zA2kYruW8YDPlWMxKDWcf9bHCQUklx41lo0+9Fe7tub+hORfIxnBrS5VvCt11mQfSeqpB9OvS9VDHXdsE4QlHss+hqBJ6W0rujjv4we2Llt2h9d4033r4LJbLLps30x5FPuPGn/h+rG7iwQ0SAI6QKLnZdEJRyc+FMbzJAvKdPhR0q9Y1G2Br9gS5IuUsoGO6AGVbBgB'}]))], created=1761048089, model='claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=48, prompt_tokens=42, total_tokens=90, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "The capital of France is Paris.\n",
      "This is a very straightforward question about geography. The capital of France is Paris. This is basic factual information that I'm confident about.\n"
     ]
    }
   ],
   "source": [
    "# Example of reasoning models - some models support reasoning_effort parameter\n",
    "response = client.chat.completions.create(\n",
    "    model=\"vertex_ai/claude-sonnet-4\",  # Try reasoning model if available\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    reasoning_effort=\"low\",  \n",
    ")\n",
    "print(\"Reasoning model response:\")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.reasoning_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning model response:\n",
      "ChatCompletion(id='chatcmpl-fab01090-75b1-4b21-9e5d-2ff6e8f2f41f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To find the average speed for the entire journey, I need to calculate the total distance and total time.\\n\\n**Given information:**\\n- First segment: 120 km in 1.5 hours\\n- Second segment: 180 km in 2 hours\\n\\n**Step 1: Calculate total distance**\\nTotal distance = 120 km + 180 km = 300 km\\n\\n**Step 2: Calculate total time**\\nTotal time = 1.5 hours + 2 hours = 3.5 hours\\n\\n**Step 3: Calculate average speed**\\nAverage speed = Total distance ÷ Total time\\nAverage speed = 300 km ÷ 3.5 hours\\nAverage speed = 85.71 km/h\\n\\nTherefore, the average speed for the entire journey is **85.71 km/h** (or 85 5/7 km/h as an exact fraction).', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content='To find the average speed for the entire journey, I need to use the formula:\\n\\nAverage speed = Total distance / Total time\\n\\nLet me break this down:\\n\\nFirst part of the journey:\\n- Distance = 120 km\\n- Time = 1.5 hours\\n\\nSecond part of the journey:\\n- Distance = 180 km\\n- Time = 2 hours\\n\\nTotal distance = 120 km + 180 km = 300 km\\nTotal time = 1.5 hours + 2 hours = 3.5 hours\\n\\nAverage speed = 300 km / 3.5 hours = 85.71... km/h\\n\\nLet me double-check this calculation:\\n300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h', thinking_blocks=[{'type': 'thinking', 'thinking': 'To find the average speed for the entire journey, I need to use the formula:\\n\\nAverage speed = Total distance / Total time\\n\\nLet me break this down:\\n\\nFirst part of the journey:\\n- Distance = 120 km\\n- Time = 1.5 hours\\n\\nSecond part of the journey:\\n- Distance = 180 km\\n- Time = 2 hours\\n\\nTotal distance = 120 km + 180 km = 300 km\\nTotal time = 1.5 hours + 2 hours = 3.5 hours\\n\\nAverage speed = 300 km / 3.5 hours = 85.71... km/h\\n\\nLet me double-check this calculation:\\n300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h', 'signature': 'ErUFCkgICBACGAIqQMKL3VamWfGDP/evvE5OjpGsLhMmlOxaSIWyizW0zEjxsb3TmJBmCgkEYyLtC3As630uYJOOu64gjpjSFt/RbbgSDG1+WTDXkZpkvX+TjRoM4h1c+aVZ7BPi9fGaIjBnLdBAcfLOFgQ/GU4akwVN/NF05/PdgB0HLDsbvcnw/U4gDLQDXEgvuP7EYZkKad8qmgQ+y12mtBNEx42OWlChd7el21uO9SlHAlL8nBEyrahmZxEjrj2vYXjiClm0xSnIaJcMuoW2KK29WXWCCwYzbenU1FZSubmkWcNXTnAkshzq6V55wyMfYSKHqrzzwRmX8EyxEG/QpDPoSUg1LDzsPKgkZhv0+1GiFC096+JvN9kyENwAw2w4d06sCQfipHtS/yRYVbQim1I07h2uswBoDRLehuTismkyeqdFLGue8TWLpvX7/Dk9M+edwhvxlaMM16ABLwC+AG2AGIS0XIwdzbVg6u3H3lfGK3jAXEdOUT0jhdf88W2i5xs4B9bzzkwPrBa8p0r0BizXpL0UU46uq0eqiiuDa0yyzpUKMesEX+OqFBh88FBe2zZ9vQlgXNENOYAgGbl6C4gdYXI9AmTEI9vUn8jbC0b54iaSc8CeA0Q8UKFVB9fMYhnr8cLwpaT7GkVNHZymoEqOHFpX/Is+93TcjtmNDvaVIx0dka3NOZOgvh9e4T/iRtvMLDJUmsoUQbVafipEoy4qhPBf3TYMdZnirkNmsKcKaDNApqoshIFtBRGS2UrWMTJmysPtAnvERJx7gDl9zCRuxw161WwxifVWI0e86yW1FnBhIUetg5bpHXZiGVFqPNca4YR6Qtns14r8GgV+MLRoPHeksnZAQHm4fLOKgc5LbQ35+kxQdPq9+OEh4IPUd2ZW2O1zFv4EOaKgqZY2vBLmI3yGGAE='}]))], created=1760959926, model='claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=424, prompt_tokens=75, total_tokens=499, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=175, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "🔄  Output: To find the average speed for the entire journey, I need to calculate the total distance and total time.\n",
      "\n",
      "**Given information:**\n",
      "- First segment: 120 km in 1.5 hours\n",
      "- Second segment: 180 km in 2 hours\n",
      "\n",
      "**Step 1: Calculate total distance**\n",
      "Total distance = 120 km + 180 km = 300 km\n",
      "\n",
      "**Step 2: Calculate total time**\n",
      "Total time = 1.5 hours + 2 hours = 3.5 hours\n",
      "\n",
      "**Step 3: Calculate average speed**\n",
      "Average speed = Total distance ÷ Total time\n",
      "Average speed = 300 km ÷ 3.5 hours\n",
      "Average speed = 85.71 km/h\n",
      "\n",
      "Therefore, the average speed for the entire journey is **85.71 km/h** (or 85 5/7 km/h as an exact fraction).\n",
      "🔄  Reasoning Content: To find the average speed for the entire journey, I need to use the formula:\n",
      "\n",
      "Average speed = Total distance / Total time\n",
      "\n",
      "Let me break this down:\n",
      "\n",
      "First part of the journey:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "Second part of the journey:\n",
      "- Distance = 180 km\n",
      "- Time = 2 hours\n",
      "\n",
      "Total distance = 120 km + 180 km = 300 km\n",
      "Total time = 1.5 hours + 2 hours = 3.5 hours\n",
      "\n",
      "Average speed = 300 km / 3.5 hours = 85.71... km/h\n",
      "\n",
      "Let me double-check this calculation:\n",
      "300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h\n"
     ]
    }
   ],
   "source": [
    "# Example of reasoning models - some models support reasoning_effort parameter\n",
    "response = client.chat.completions.create(\n",
    "    model=\"vertex_ai/claude-sonnet-4\",  # Try reasoning model if available\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"If a train travels 120 km in 1.5 hours, and then travels another 180 km in 2 hours, what is the average speed for the entire journey??\"},\n",
    "    ],\n",
    "    reasoning_effort=\"high\",  \n",
    ")\n",
    "print(\"Reasoning model response:\")\n",
    "print(response)\n",
    "print(f'🔄  Output: '+response.choices[0].message.content)\n",
    "print(f'🔄  Reasoning Content: '+response.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 A little advice on prompting\n",
    "\n",
    "There are some differences to consider when prompting a reasoning model. \n",
    "\n",
    "- Reasoning models provide better results on tasks with only high-level guidance, while GPT models often benefit from very precise instructions.\n",
    "-  A reasoning model is like a senior co-worker—you can give them a goal to achieve and trust them to work out the details.\n",
    "- A GPT model is like a junior coworker—they'll perform best with explicit instructions to create a specific output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAG - Chat with Your Data {#rag}\n",
    "\n",
    "Retrieval Augmented Generation allows LLMs to access and reason over external knowledge.\n",
    "\n",
    "It consists of two steps: \n",
    "- retrieval/search (mostly embedding-based) and \n",
    "- augmented generation using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❓ Question: What is the capital of Norway?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. The capital of Norway is Oslo.\n",
      "  3. Norway has a population of approximately 5.4 million people.\n",
      "🤖 Answer: Oslo\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: Tell me about Norwegian languages\n",
      "📚 Retrieved docs: 3\n",
      "  1. The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\n",
      "  2. Norway is not a member of the European Union but is part of the EEA.\n",
      "  3. Machine learning is a subset of artificial intelligence.\n",
      "🤖 Answer: Norway’s official language is Norwegian, which has two written forms: Bokmål and Nynorsk.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: What is Python programming language?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. The capital of Norway is Oslo.\n",
      "  3. The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\n",
      "🤖 Answer: The provided context doesn’t include information about the Python programming language, so I can’t answer the question based on it.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: How many people live in Sweden?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. Norway has a population of approximately 5.4 million people.\n",
      "  3. The country has significant oil and gas reserves in the North Sea.\n",
      "🤖 Answer: The context doesn’t provide information about Sweden’s population, so I can’t answer that from the given details.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# Simple RAG example with in-memory knowledge base\n",
    "knowledge_base = {\n",
    "    \"norway_facts\": [\n",
    "        \"Norway is a Scandinavian country in Northern Europe.\",\n",
    "        \"The capital of Norway is Oslo.\",\n",
    "        \"Norway has a population of approximately 5.4 million people.\",\n",
    "        \"The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\",\n",
    "        \"Norway is famous for its fjords, northern lights, and midnight sun.\",\n",
    "        \"The country has significant oil and gas reserves in the North Sea.\",\n",
    "        \"Norway is not a member of the European Union but is part of the EEA.\"\n",
    "    ],\n",
    "    \"tech_facts\": [\n",
    "        \"Python is a high-level programming language created by Guido van Rossum.\",\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"APIs (Application Programming Interfaces) allow different software systems to communicate.\",\n",
    "        \"Cloud computing provides on-demand access to computing resources.\",\n",
    "        \"Git is a distributed version control system.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def simple_retrieval(query: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for category, docs in knowledge_base.items():\n",
    "        for doc in docs:\n",
    "            # Simple keyword matching\n",
    "            if any(word in doc.lower() for word in query_lower.split()):\n",
    "                relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs[:k]\n",
    "\n",
    "def rag_query(user_question: str) -> str:\n",
    "    \"\"\"Perform RAG: retrieve relevant docs and generate answer.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    relevant_docs = simple_retrieval(user_question)\n",
    "    \n",
    "    # Step 2: Create context from retrieved documents\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc in relevant_docs])\n",
    "    \n",
    "    # Step 3: Generate answer using context\n",
    "    rag_prompt = f\"\"\"\n",
    "    Answer the following question using the provided context. If the context doesn't contain \n",
    "    enough information to answer the question, say so.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {user_question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-5\",\n",
    "        messages=[{\"role\": \"user\", \"content\": rag_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content, relevant_docs\n",
    "\n",
    "# Test RAG system\n",
    "questions = [\n",
    "    \"What is the capital of Norway?\",\n",
    "    \"Tell me about Norwegian languages\",\n",
    "    \"What is Python programming language?\",\n",
    "    \"How many people live in Sweden?\"  # This shows limited knowledge\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    answer, docs = rag_query(question)\n",
    "    print(f\"📚 Retrieved docs: {len(docs)}\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"  {i}. {doc}\")\n",
    "    print(f\"🤖 Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-based RAG (Conceptual)\n",
    "\n",
    "In practice, RAG systems use vector embeddings for more sophisticated retrieval:\n",
    "\n",
    "Typical RAG process: \n",
    "\n",
    "1. Document Processing:\n",
    "   - Split documents into chunks\n",
    "   - Generate embeddings for each chunk\n",
    "   - Store in vector database (e.g., Pinecone, Weaviate, ChromaDB)\n",
    "\n",
    "2. Query Processing:\n",
    "   - Generate embedding for user question\n",
    "   - Find similar document chunks using a similarity metric, e.g. cosine similarity\n",
    "   - Retrieve top-k most relevant chunks\n",
    "\n",
    "3. Generation:\n",
    "   - Combine retrieved chunks into context\n",
    "   - Generate answer using LLM + context\n",
    "   - Optionally include source citations\n",
    "\n",
    "🔧 Tools for Production RAG:\n",
    "- LangChain / LlamaIndex for orchestration\n",
    "- OpenAI/Cohere embeddings for vectors\n",
    "- Vector databases for storage\n",
    "- Chunking strategies for optimal retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "### Langchain example \n",
    "# Taken from https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "\n",
    "\n",
    "# Assuming your LiteLLM Proxy is running on localhost:4000\n",
    "llm = ChatOpenAI(\n",
    "    model=\"azure/gpt-4o\", # or any model configured in your LiteLLM Proxy\n",
    "    temperature=0,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",  base_url=base_url)\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate]) #We'll use LangGraph to tie together the retrieval and generation steps into a single application\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down complex tasks into smaller, simpler steps or subgoals. It can be achieved through techniques like chain of thought prompting, which instructs models to think step by step, or by using task-specific instructions and human inputs. This approach helps in managing and interpreting the model's thinking process for better performance on complex tasks.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know who Paul is based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who is Paul?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agents {#agents}\n",
    "\n",
    "AI agents can make decisions, use tools, and take actions to accomplish goals with minimal human intervention. \n",
    "\n",
    "Often, specialized agents for different tasks interact together in a multi-agent system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 🎯 Scenario\n",
    "We'll create a simple content creation pipeline with two specialized agents:\n",
    "- **Research Agent**: Finds and analyzes information\n",
    "- **Writer Agent**: Creates content based on research findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base agent class\"\"\"\n",
    "    def __init__(self, name, role, client, model=\"azure/gpt-4o\"):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "    \n",
    "    def generate_response(self, prompt, system_prompt=None):\n",
    "        \"\"\"Generate a response using the LLM\"\"\"\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent(Agent):\n",
    "    \"\"\"Agent specialized in research and fact-finding\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        super().__init__(\"Research Agent\", \"researcher\", client)\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a research specialist. Your job is to:\n",
    "        1. Analyze topics and identify key research areas\n",
    "        2. Provide structured, factual information\n",
    "        3. Suggest reliable sources when possible\n",
    "        4. Keep responses concise but comprehensive\n",
    "        \n",
    "        Always format your research as:\n",
    "        - Key Facts: (bullet points)\n",
    "        - Important Context: (brief explanation)\n",
    "        - Suggested Focus: (what to emphasize)\n",
    "        \"\"\"\n",
    "    \n",
    "    def research_topic(self, topic):\n",
    "        \"\"\"Research a given topic\"\"\"\n",
    "        prompt = f\"Research the topic: {topic}\"\n",
    "        print(f\"🔍 {self.name} researching: {topic}\")\n",
    "        return self.generate_response(prompt, self.system_prompt)\n",
    "\n",
    "\n",
    "class WriterAgent(Agent):\n",
    "    \"\"\"Agent specialized in content creation\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        super().__init__(\"Writer Agent\", \"writer\", client)\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a skilled content writer. Your job is to:\n",
    "        1. Create engaging, well-structured content\n",
    "        2. Use research findings effectively\n",
    "        3. Write in a clear, accessible style\n",
    "        4. Include practical examples when relevant\n",
    "        \n",
    "        Always structure your content with:\n",
    "        - Clear introduction\n",
    "        - Main points with examples\n",
    "        - Practical conclusion\n",
    "        \"\"\"\n",
    "    \n",
    "    def write_content(self, topic, research_findings):\n",
    "        \"\"\"Write content based on research findings\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Topic: {topic}\n",
    "        \n",
    "        Research findings:\n",
    "        {research_findings}\n",
    "        \n",
    "        Write a comprehensive but concise article (300-400 words) based on this research.\n",
    "        \"\"\"\n",
    "        print(f\"✍️ {self.name} writing content about: {topic}\")\n",
    "        return self.generate_response(prompt, self.system_prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Multi-agent content pipeline initialized!\n"
     ]
    }
   ],
   "source": [
    "class MultiAgentContentPipeline:\n",
    "    \"\"\"Coordinates multiple agents to create content\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.research_agent = ResearchAgent(client)\n",
    "        self.writer_agent = WriterAgent(client)\n",
    "    \n",
    "    def create_content(self, topic):\n",
    "        \"\"\"Complete content creation pipeline\"\"\"\n",
    "        print(f\"🚀 Starting content creation pipeline for: {topic}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Research phase\n",
    "        research_findings = self.research_agent.research_topic(topic)\n",
    "        print(\"\\n📋 Research Results:\")\n",
    "        print(research_findings)\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        \n",
    "        # Step 2: Writing phase\n",
    "        final_content = self.writer_agent.write_content(topic, research_findings)\n",
    "        print(\"\\n📝 Final Content:\")\n",
    "        print(final_content)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"research\": research_findings,\n",
    "            \"content\": final_content\n",
    "        }\n",
    "\n",
    "# Create the multi-agent system\n",
    "pipeline = MultiAgentContentPipeline(client)\n",
    "print(\"🤖 Multi-agent content pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting content creation pipeline for: How artificial intelligence is changing journalism\n",
      "============================================================\n",
      "🔍 Research Agent researching: How artificial intelligence is changing journalism\n",
      "\n",
      "📋 Research Results:\n",
      "- **Key Facts:**\n",
      "  - AI is used in newsrooms for automating content creation, such as generating news summaries, sports reports, and financial updates. Examples include The Washington Post's Heliograf and Bloomberg's Cyborg.\n",
      "  - AI tools help in data analysis for investigative journalism by identifying patterns and connections that may be missed by human researchers.\n",
      "  - AI-driven algorithms curate personalized news feeds, influencing how audiences consume information.\n",
      "  - AI assists in content verification efforts, helping to identify misinformation and deepfakes.\n",
      "\n",
      "- **Important Context:**\n",
      "  - The integration of AI in journalism is driven by the need for efficiency and accuracy in a rapidly evolving news landscape. AI can handle large datasets and automate repetitive tasks, allowing journalists to focus on more nuanced reporting.\n",
      "  - While AI enhances productivity, it also raises ethical concerns, such as algorithmic bias and the potential loss of jobs in journalism. Ensuring transparency and maintaining journalistic integrity are key challenges.\n",
      "  - The role of AI in journalism is expanding as technology advances, with potential applications in virtual reality storytelling and audience engagement through chatbots.\n",
      "\n",
      "- **Suggested Focus:**\n",
      "  - Emphasize the balance between leveraging AI for efficiency and maintaining ethical standards in journalism.\n",
      "  - Explore the impact of AI on the quality and credibility of news content, considering both the benefits and potential downsides.\n",
      "  - Investigate the future landscape of journalism with AI, including emerging technologies and their implications for news consumption and production.\n",
      "\n",
      "For further reading, consider sources like the Reuters Institute for the Study of Journalism, Nieman Lab, and academic journals on media and technology.\n",
      "\n",
      "----------------------------------------\n",
      "✍️ Writer Agent writing content about: How artificial intelligence is changing journalism\n",
      "\n",
      "📝 Final Content:\n",
      "### How Artificial Intelligence is Revolutionizing Journalism\n",
      "\n",
      "In the ever-evolving landscape of journalism, artificial intelligence (AI) is emerging as a transformative force. From automating content creation to enhancing content verification, AI is reshaping how news is produced and consumed. This article delves into the multifaceted impact of AI on journalism, highlighting both the opportunities it presents and the challenges it poses.\n",
      "\n",
      "AI's integration into newsrooms is primarily driven by the quest for efficiency and accuracy. Leading news organizations like The Washington Post and Bloomberg have pioneered the use of AI tools such as Heliograf and Cyborg to automate the generation of news summaries, sports reports, and financial updates. These tools not only save time but also allow journalists to direct their efforts towards more nuanced and investigative reporting. For instance, AI's capacity to analyze vast datasets and identify patterns has proven invaluable in investigative journalism, uncovering connections that might elude human researchers.\n",
      "\n",
      "Beyond content creation, AI significantly influences how audiences consume news. AI-driven algorithms curate personalized news feeds, tailoring content to individual preferences and thus increasing engagement. However, this personalization raises questions about the quality and credibility of news content, as algorithms may inadvertently reinforce biases or create echo chambers.\n",
      "\n",
      "Another critical role of AI in journalism is in content verification. In an era rife with misinformation and deepfakes, AI assists journalists in verifying facts and identifying misleading content, thereby safeguarding the integrity of the news. However, this technological advancement also brings ethical concerns to the fore. The potential for algorithmic bias and the fear of job losses in the journalism industry are significant issues that need addressing. Ensuring transparency in AI processes and maintaining journalistic integrity remain paramount.\n",
      "\n",
      "Looking ahead, AI's role in journalism is set to expand with advancements in technology. Concepts such as virtual reality storytelling and audience engagement through chatbots are on the horizon, promising to revolutionize how news is both produced and consumed. However, striking a balance between leveraging AI for efficiency and upholding ethical standards is crucial for the future of journalism.\n",
      "\n",
      "In conclusion, while AI offers remarkable benefits in terms of efficiency and accuracy, it also presents challenges that must be navigated carefully. The future of journalism will depend on how well the industry can integrate AI while preserving the core values of credibility and integrity. For those interested in exploring this topic further, resources such as the Reuters Institute for the Study of Journalism and Nieman Lab provide valuable insights into the intersection of media and technology.\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Let's see agent collaboration on a complex topic\n",
    "result1 = pipeline.create_content(\"How artificial intelligence is changing journalism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting content creation pipeline for: Norwegian work-life balance culture\n",
      "============================================================\n",
      "🔍 Research Agent researching: Norwegian work-life balance culture\n",
      "\n",
      "📋 Research Results:\n",
      "- **Key Facts:**\n",
      "  - Norway consistently ranks high in global work-life balance studies.\n",
      "  - Typical workweek is 37.5 hours, with flexible working hours often available.\n",
      "  - Generous parental leave policies, with both parents entitled to paid leave.\n",
      "  - Employees receive five weeks of paid vacation annually.\n",
      "  - Strong labor unions advocate for workers' rights and work-life balance.\n",
      "  - A significant emphasis on family time and leisure activities.\n",
      "\n",
      "- **Important Context:**\n",
      "  - The work-life balance culture in Norway is influenced by its social democratic policies, which prioritize employee welfare and gender equality.\n",
      "  - The \"Janteloven\" cultural concept emphasizes modesty and equality, discouraging overachievement at the expense of personal life.\n",
      "  - Norway's high standard of living and robust welfare system support a balanced lifestyle, reducing stress related to job security and health care.\n",
      "\n",
      "- **Suggested Focus:**\n",
      "  - Examine the impact of Norway’s labor laws and welfare policies on work-life balance.\n",
      "  - Explore the cultural values that underpin attitudes towards work and leisure.\n",
      "  - Analyze how Norway’s work-life balance practices compare to other countries in the Nordic region and beyond.\n",
      "  - Investigate the role of technology and remote work in maintaining work-life balance in Norway.\n",
      "\n",
      "Sources for further research:\n",
      "- OECD Better Life Index\n",
      "- Statistics Norway (SSB)\n",
      "- Norwegian Labour and Welfare Administration (NAV)\n",
      "\n",
      "----------------------------------------\n",
      "✍️ Writer Agent writing content about: Norwegian work-life balance culture\n",
      "\n",
      "📝 Final Content:\n",
      "**Norwegian Work-Life Balance Culture: A Blueprint for Well-Being**\n",
      "\n",
      "Norway has long been heralded as a beacon of work-life balance, consistently ranking high in global studies. This reputation is well-deserved, as the country offers a lifestyle that effectively harmonizes professional obligations with personal well-being. Understanding the framework of Norway’s work-life balance culture provides valuable insights into creating a more balanced lifestyle.\n",
      "\n",
      "At the heart of Norway’s successful work-life equilibrium are its labor laws and welfare policies. The typical workweek in Norway is 37.5 hours, with many organizations offering flexible working hours. This flexibility allows employees to better juggle work responsibilities with personal commitments. Moreover, Norway’s labor laws grant employees five weeks of paid vacation annually, recognizing the importance of rest and recuperation. Generous parental leave policies ensure that both parents can take paid leave, fostering a family-friendly work environment. These benefits are staunchly protected by strong labor unions, which advocate for workers' rights and emphasize the importance of balance.\n",
      "\n",
      "Cultural values also play a significant role in shaping Norway's approach to work and leisure. The concept of \"Janteloven,\" which discourages excessive individualism and overachievement, aligns with the social democratic policies that prioritize employee welfare and gender equality. This cultural ethos encourages Norwegians to pursue a balanced life, valuing leisure and family time as much as professional success. This emphasis on modesty and equality ensures that personal life is not sacrificed for career advancement.\n",
      "\n",
      "Norway’s high standard of living and robust welfare system further support a balanced lifestyle. With reduced stress related to job security and health care, employees can focus on spending quality time with family and engaging in leisure activities. This focus on well-being is complemented by the increasing role of technology and remote work. The digital age has allowed many Norwegians to maintain their work-life balance, as remote work options provide the flexibility to meet both professional and personal needs.\n",
      "\n",
      "When compared to other countries, particularly within the Nordic region, Norway’s practices offer a compelling model. While other nations also prioritize work-life balance, Norway's combination of supportive policies, cultural values, and technological integration sets it apart. For countries seeking to improve their work-life balance culture, Norway provides a blueprint that emphasizes the synergy between work obligations and personal well-being.\n",
      "\n",
      "In conclusion, Norway’s work-life balance culture is the product of thoughtful policies, cultural values, and technological advancements that collectively promote a fulfilling lifestyle. By prioritizing employee welfare and fostering environments that value personal time, Norway sets a standard that other nations can emulate to enhance their own work-life balance frameworks.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Norwegian culture topic\n",
    "result2 = pipeline.create_content(\"Norwegian work-life balance culture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Popular Agent Frameworks:\n",
    "\n",
    "- [LangChain/LangGraph](https://www.langchain.com/)\n",
    "- [AutoGen (Microsoft)](https://microsoft.github.io/autogen/)\n",
    "- [AutoGPT](https://agpt.co/)\n",
    "- [CrewAI](https://www.crewai.com/)\n",
    "- [Microsoft Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)\n",
    "- [Llamaindex](https://www.llamaindex.ai/)\n",
    "- [Smolagents](https://huggingface.co/docs/smolagents/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Further reading\n",
    "\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
