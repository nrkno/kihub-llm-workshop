{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Large Language Models Workshop\n",
    "\n",
    "Welcome to our workshop on large language models, in which we will mainly focus on text generation models (decoder-based architecture, like ChatGPT).\n",
    "\n",
    "\n",
    "## 📚 Table of Contents\n",
    "\n",
    "| Section | Topic | Description |\n",
    "|---------|-------|-------------|\n",
    "| **1** | [Basic Chat Completions](#setup) | 🎯 Getting started  |\n",
    "| **2** | [Parameters & Configuration](#parameters) | ⚙️ Tuning model behavior and output |\n",
    "| **3** |  [Prompt Engineering](#prompt-engineering) | 🎨 How to craft effective prompts to get the best results from LLMs|\n",
    "| **4** | [Structured Output](#structured-output) | 📊 Getting reliable JSON responses |\n",
    "| **5** | [Function Calling](#function-calling) | 🔧 Connecting LLMs to external tools |\n",
    "| **6** | [Streaming Output](#streaming) | ⚡ Real-time responses for better UX |\n",
    "| **7** | [Reasoning Models](#reasoning-models) | 🧠 Advanced models that \"think\" step-by-step |\n",
    "| **8** | [RAG - Chat with Your Data](#rag) | 📖 Making LLMs work with your documents |\n",
    "| **9** | [Agents](#agents) | 🤖 Autonomous AI that can plan and execute tasks |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Icebreaker: 20 Questions Game\n",
    "\n",
    "\n",
    "### 🎮 Let's Play a Game!\n",
    "\n",
    "Welcome to our interactive **~~20~~ 10\n",
    "Questions** game! <br>\n",
    "This is a fun way to start exploring what LLMs can do while demonstrating some key concepts we'll cover in this workshop.\n",
    "\n",
    "> **🎯 The Challenge:** GPT will think of something **Norway-related** and you have 10 yes/no questions to guess what it is!\n",
    "\n",
    "### 📋 How to Play:\n",
    "\n",
    "| Step | Action | Command |\n",
    "|------|--------|---------|\n",
    "| 🚀 | **Start the game** | `game.start_game()` |\n",
    "| ❓ | **Ask questions** | `game.ask_question(\"Is it alive?\")` |\n",
    "| 📊 | **Check status** | `game.get_status()` |\n",
    "\n",
    "\n",
    "As part of the demo, we get introduced to some LLM-related concepts:\n",
    "\n",
    "-  🎨 **Prompting** - How to give instructions to the model\n",
    "- 🧠 **Conversation History** - The AI remembers everything you've asked\n",
    "- 📊 **Structured Output** - Uses JSON to reliably track game state  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 20 Questions Game Ready!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# LiteLLM Configuration for the game\n",
    "api_key = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "base_url = \"https://litellm.plattform-int.k8s.ma.nrk.cloud\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "# Structured output models for the game\n",
    "class GameResponse(BaseModel):\n",
    "    \"\"\"Structured response for game interactions using Pydantic.\"\"\"\n",
    "    answer: str  # \"yes\", \"no\", \"sometimes\", \"sort of\"\n",
    "    is_correct_guess: bool  # True if user guessed correctly\n",
    "    game_over: bool  # True if game should end\n",
    "    human_readable_response: str  # What to show to the user\n",
    "    secret_revealed: Optional[str] = None  # What the AI was thinking of (only if game_over=True)\n",
    "\n",
    "class TwentyQuestionsGameStructured:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model = \"azure/gpt-4o\"  # Use gpt-4o for structured output parsing\n",
    "        self.conversation_history = []\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions = 10\n",
    "        self.game_active = False\n",
    "        self.secret_thing = None\n",
    "    \n",
    "    def start_game(self):\n",
    "        \"\"\"Initialize a new game with GPT thinking of something.\"\"\"\n",
    "        \n",
    "        # Reset game state\n",
    "        self.conversation_history = []\n",
    "        self.questions_asked = 0\n",
    "        self.game_active = True\n",
    "        \n",
    "        # Have GPT think of something and get initial response\n",
    "        setup_prompt = \"\"\"\n",
    "        You are about to play 10 Questions! Please think of something for the human to guess. \n",
    "        It can be:\n",
    "        - An animal, object, person, place, concept, food, movie, book, etc.\n",
    "        - Something well-known that most people would recognize\n",
    "        - Not too obscure or overly specific\n",
    "        - Make it Norway-related to make it more interesting for the workshop!\n",
    "        \n",
    "        Respond with a structured JSON indicating the game has started.\n",
    "        Remember what you chose throughout our conversation. Be consistent with your answers.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are playing 10 Questions. Think of something Norwegian-related and respond with structured output.\"},\n",
    "            {\"role\": \"user\", \"content\": setup_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Use structured output with Pydantic\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format=GameResponse\n",
    "        )\n",
    "        \n",
    "        # Parse the structured response\n",
    "        game_data = GameResponse.model_validate_json(response.choices[0].message.content)\n",
    "        \n",
    "        # Store the conversation context\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are playing 10 Questions. You have thought of something Norwegian-related. Answer questions consistently and use structured JSON responses.\"},\n",
    "            {\"role\": \"user\", \"content\": setup_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "        ]\n",
    "        \n",
    "        print(\"🎮 Game Started!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"I'm thinking of something Norwegian! You have 10 yes/no questions to guess what it is. Ask away!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Questions remaining: {self.max_questions}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def ask_question(self, question):\n",
    "        \"\"\"Ask a question in the game using structured output.\"\"\"\n",
    "        \n",
    "        if not self.game_active:\n",
    "            return \"❌ No game is currently active. Please start a new game first!\"\n",
    "        \n",
    "        if self.questions_asked >= self.max_questions:\n",
    "            return f\"❌ You've used all {self.max_questions} questions! The game is over.\"\n",
    "        \n",
    "        self.questions_asked += 1\n",
    "        \n",
    "        # Create prompt for structured response\n",
    "        structured_prompt = f\"\"\"\n",
    "        Question {self.questions_asked}: {question}\n",
    "        \n",
    "        Please respond with structured JSON containing:\n",
    "        - answer: \"yes\", \"no\", \"sometimes\", or \"sort of\" \n",
    "        - is_correct_guess: true if they guessed exactly what you're thinking of\n",
    "        - game_over: true if they guessed correctly OR if this was question 20\n",
    "        - human_readable_response: a friendly response to show the user\n",
    "        - secret_revealed: only include this if game_over is true - reveal what you were thinking of\n",
    "        \n",
    "        Remember to be consistent with what you originally chose to think of!\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": structured_prompt})\n",
    "        \n",
    "        # Get structured response\n",
    "        response = self.client.chat.completions.parse(\n",
    "            model=self.model,\n",
    "            messages=self.conversation_history,\n",
    "            response_format=GameResponse\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        game_data = GameResponse.model_validate_json(response.choices[0].message.content)\n",
    "        \n",
    "        # Add response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "        \n",
    "        # Process the structured response\n",
    "        if game_data.is_correct_guess:\n",
    "            self.game_active = False\n",
    "            result = f\"🎉 CONGRATULATIONS! You guessed it in {self.questions_asked} questions!\\n\\n\"\n",
    "            result += f\"🤖 GPT: {game_data.human_readable_response}\\n\"\n",
    "            if game_data.secret_revealed:\n",
    "                result += f\"🎭 The answer was: {game_data.secret_revealed}\"\n",
    "            return result\n",
    "        \n",
    "        # Format the regular response\n",
    "        result = f\"❓ Question {self.questions_asked}/{self.max_questions}: {question}\\n\"\n",
    "        result += f\"🤖 GPT: {game_data.human_readable_response}\\n\"\n",
    "        result += f\"📊 Questions remaining: {self.max_questions - self.questions_asked}\"\n",
    "        \n",
    "        # Check if game should end (reached max questions)\n",
    "        if game_data.game_over or self.questions_asked >= self.max_questions:\n",
    "            self.game_active = False\n",
    "            result += \"\\n\\n💀 Game Over! You've used all your questions.\"\n",
    "            \n",
    "            if game_data.secret_revealed:\n",
    "                result += f\"\\n🎭 The answer was: {game_data.secret_revealed}\"\n",
    "            else:\n",
    "                # Force reveal if not provided\n",
    "                reveal_prompt = \"Game over! Please reveal what you were thinking of.\"\n",
    "                self.conversation_history.append({\"role\": \"user\", \"content\": reveal_prompt})\n",
    "                \n",
    "                reveal_response = self.client.chat.completions.parse(\n",
    "                    model=self.model,\n",
    "                    messages=self.conversation_history,\n",
    "                    response_format=GameResponse\n",
    "                )\n",
    "                \n",
    "                reveal_data = GameResponse.model_validate_json(reveal_response.choices[0].message.content)\n",
    "                if reveal_data.secret_revealed:\n",
    "                    result += f\"\\n🎭 The answer was: {reveal_data.secret_revealed}\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current game status.\"\"\"\n",
    "        if not self.game_active:\n",
    "            return \"No active game. Start a new game to play!\"\n",
    "        \n",
    "        return f\"🎮 Game in progress: {self.questions_asked}/{self.max_questions} questions asked\"\n",
    "\n",
    "# Create improved game instance\n",
    "game = TwentyQuestionsGameStructured(client)\n",
    "\n",
    "print(\"🎯 20 Questions Game Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Game Started!\n",
      "==================================================\n",
      "I'm thinking of something Norwegian! You have 20 yes/no questions to guess what it is. Ask away!\n",
      "==================================================\n",
      "Questions remaining: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new game\n",
    "game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Question 5/5: Is it a female person?\n",
      "🤖 GPT: No, it's not a female person. Keep guessing!\n",
      "📊 Questions remaining: 0\n",
      "\n",
      "💀 Game Over! You've used all your questions.\n",
      "🎭 The answer was: Roald Amundsen, the famous Norwegian explorer.\n"
     ]
    }
   ],
   "source": [
    "# Ask questions one by one - GPT-5 will remember the conversation!\n",
    "# Example:\n",
    "print(game.ask_question(\"Is it a female person?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🎯 Basic Chat Completions {#setup}\n",
    "\n",
    "Let's dive in with your first LLM interaction! We'll start simple by just testing one of LiteLLM's models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LiteLLM client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "api_key = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "base_url = \"https://litellm.plattform-int.k8s.ma.nrk.cloud\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "print(\"✅ LiteLLM client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🛠️ What is LiteLLM?\n",
    "\n",
    "**LiteLLM** LiteLLM is a unified interface that allows you to call different LLM providers (OpenAI, Anthropic, Azure, Google, etc.) using the OpenAI format. \n",
    "\n",
    "In our setup, LiteLLM acts as a proxy that translates OpenAI-formatted requests to work with various model providers, making it easy to experiment with different models without changing our code.\n",
    "\n",
    "\n",
    "### 🏢 NRK's LiteLLM Setup\n",
    "\n",
    "| Resource | Link |\n",
    "|----------|------|\n",
    "| 🌐 **Instance** | https://litellm.plattform-int.k8s.ma.nrk.cloud |\n",
    "| 📚 **Documentation** | [Confluence Link](https://nrkconfluence.atlassian.net/wiki/spaces/Kihub/pages/2578284720/LiteLLM) |\n",
    "\n",
    "If you need an API key for a specific project/team, just contact us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat Completion\n",
    "\n",
    "Let's start with the simplest possible interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Oslo.\n"
     ]
    }
   ],
   "source": [
    "# Basic chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Norway?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CO3ckRla08fksUwI4n1l4vSAI7t5Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The image shows a 3D-animated Antarctic scene. In the foreground a gray-and-white penguin chick leaps joyfully with wings spread and mouth open, casting a shadow on the snow. In the background, many emperor penguins stand and waddle near the edge of an icy shoreline, with blue sky, scattered clouds, and a tall ice shelf stretching across the horizon. The mood is playful, reminiscent of the “Happy Feet” style.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), provider_specific_fields={'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}})], created=1759849838, model='gpt-5-2025-08-07', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=356, prompt_tokens=641, total_tokens=997, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=256, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 1, 'content_filter_result': {'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}, 'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'custom_blocklists': {'filtered': False, 'details': []}}}, {'prompt_index': 0, 'content_filter_result': {}}])\n",
      "The image shows a 3D-animated Antarctic scene. In the foreground a gray-and-white penguin chick leaps joyfully with wings spread and mouth open, casting a shadow on the snow. In the background, many emperor penguins stand and waddle near the edge of an icy shoreline, with blue sky, scattered clouds, and a tall ice shelf stretching across the horizon. The mood is playful, reminiscent of the “Happy Feet” style.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Helper function to encode images to base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Example with image or PDF file\n",
    "base64_file = encode_image(\"./data/penguin.jpeg\") \n",
    "response_with_file = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the content of the attached file.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_file}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response_with_file)\n",
    "print(response_with_file.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameters and Configuration {#parameters}\n",
    "\n",
    "LLMs have various parameters that control their behavior:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Controls randomness and creativity (0.0 = more deterministic, 1.0 = very creative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌡️  Temperature 0.0:\n",
      "In the bustling city of New Metropolis, where skyscrapers kissed the clouds and neon lights painted the night, there existed a small, unassuming workshop tucked away in a quiet alley. This was the home of Professor Elara Finch, a renowned roboticist known for her eccentric inventions. Her latest creation, a robot named Arti, was designed with a singular purpose: to learn and create art.\n",
      "\n",
      "Arti was unlike any other robot. With a sleek, humanoid form and a face that could\n",
      "--------------------------------------------------\n",
      "\n",
      "🌡️  Temperature 0.5:\n",
      "In a small, bustling city where technology and tradition coexisted in harmony, there was a quaint little art studio nestled between a bakery that wafted scents of fresh bread and a bookstore filled with the whispers of turning pages. The studio, owned by an eccentric artist named Clara, was a haven for creativity, its walls adorned with vibrant canvases that told stories of distant lands and fantastical dreams.\n",
      "\n",
      "Clara had always been fascinated by the idea of merging technology with art. One day, she\n",
      "--------------------------------------------------\n",
      "\n",
      "🌡️  Temperature 1.0:\n",
      "In a bustling city of the near future, where skyscrapers pierced the sky and hovercars flitted about like metallic hummingbirds, dwelled a unique institution—ARTEX, an avant-garde art studio intertwined deeply with technology. Within its high-tech sanctuary, robots and artists collaborated, exploring new dimensions of creativity.\n",
      "\n",
      "Amongst these technological beings was a robot named VERA, short for Visual Expression and Rendering Automaton. VERA was designed with sleek silver plating and articulate limbs, optimized for precision and dex\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"Write a creative story about a robot learning to paint.\"\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🌡️  Temperature {temp}:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "\n",
    "Controls the maximum length of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Max tokens 50:\n",
      "Machine learning is a way for computers to learn and make decisions without being explicitly programmed. Imagine teaching a computer to recognize photos of cats. Instead of telling the computer exactly what makes a picture a cat, you give it lots of pictures labeled \"cat\"\n",
      "Actual tokens used: 50\n",
      "--------------------------------------------------\n",
      "\n",
      "📏 Max tokens 150:\n",
      "Machine learning is a way for computers to learn from experience and make predictions or decisions without being explicitly programmed to perform the task. Imagine teaching a child to recognize fruits by showing many examples of each type. Over time, without directly programming the child to know every fruit, they start to identify apples and oranges just from seeing enough examples. Similarly, in machine learning, you feed data into algorithms, and they identify patterns or make decisions based on this data.\n",
      "\n",
      "Here's a simplified breakdown:\n",
      "\n",
      "1. **Data Input**: You provide the machine with data, which is like information or examples (e.g., images of cats and dogs).\n",
      "\n",
      "2. **Learning**: The machine uses algorithms, which are sets of rules or instructions, to find patterns in the data\n",
      "Actual tokens used: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "📏 Max tokens 300:\n",
      "Machine learning is a way for computers to learn from examples and improve their performance over time without being explicitly programmed for specific tasks. Think of it like teaching a child to recognize different animals. Instead of giving the child a list of rules to identify each animal, you show them lots of pictures of animals along with their names. Over time, the child learns to recognize patterns and can identify animals even in new pictures they haven't seen before.\n",
      "\n",
      "In the same way, you teach a computer using data. You provide it with lots of examples, and through processes that mimic learning, the computer figures out how to predict or classify new data. This method can be used for various tasks, like recommending movies, recognizing speech, or detecting spam emails. Essentially, machine learning allows computers to \"learn\" by finding patterns in data.\n",
      "Actual tokens used: 163\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Different max_tokens settings\n",
    "prompt = \"Explain machine learning in simple terms.\"\n",
    "\n",
    "token_limits = [50, 150, 300]\n",
    "\n",
    "for max_tokens in token_limits:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📏 Max tokens {max_tokens}:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"Actual tokens used: {response.usage.completion_tokens}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation History\n",
    "\n",
    "Maintaining context across multiple exchanges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response:\n",
      "Python is a high-level, interpreted, general-purpose programming language known for its readability and ease of use. Created by Guido van Rossum and first released in 1991, it emphasizes clear, expressive code and a “batteries-included” standard library.\n",
      "\n",
      "Key points:\n",
      "- Multi-paradigm: supports procedural, object-oriented, and functional styles.\n",
      "- Readable syntax: significant indentation instead of braces; clean, concise code.\n",
      "- Dynamically typed with optional static type hints (PEP 484) for better tooling.\n",
      "- Cross-platform and open-source (PSF license), with an active community and PEP-based governance.\n",
      "- Rich ecosystem: pip and PyPI for packages; powerful libraries for web (Django, Flask), data science/ML (NumPy, pandas, SciPy, scikit-learn, PyTorch, TensorFlow), automation/scripting, DevOps, and more.\n",
      "- Standard tools: venv/virtualenv for environments; pip/poetry/pipenv for dependency management; conda as an alternative distribution and package manager.\n",
      "- Implementations: CPython (reference, most used), PyPy (JIT for speed), MicroPython/CircuitPython (embedded), Jython (JVM), IronPython (.NET).\n",
      "- Performance model: CPython compiles to bytecode executed by a VM; has a Global Interpreter Lock (GIL) affecting multi-threaded CPU-bound code. Concurrency via multiprocessing, asyncio, or C-extensions; heavy numeric work leverages optimized native libraries (e.g., NumPy).\n",
      "- Current versions are Python 3.x (Python 2 reached end-of-life in 2020).\n",
      "\n",
      "In practice, Python is used for web development, data analysis, machine learning, automation/scripting, scientific computing, testing, tooling, and education due to its gentle learning curve and powerful libraries.\n"
     ]
    }
   ],
   "source": [
    "# Conversation with history\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in programming.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "# First exchange\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=conversation\n",
    ")\n",
    "\n",
    "print(\"First response:\")\n",
    "print(response1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second response (with context):\n",
      "Here are two tiny Python examples:\n",
      "\n",
      "Example 1: the classic\n",
      "print(\"Hello, world!\")\n",
      "\n",
      "Example 2: a small script using a function, a list, a list comprehension, a dict, and basic math\n",
      "def greet(name):\n",
      "    return f\"Hello, {name}!\"\n",
      "\n",
      "def main():\n",
      "    numbers = [1, 2, 3, 4, 5]\n",
      "\n",
      "    # List comprehension: keep only even numbers\n",
      "    evens = [n for n in numbers if n % 2 == 0]\n",
      "\n",
      "    # Dict comprehension: map each number to its square\n",
      "    squares = {n: n * n for n in numbers}\n",
      "\n",
      "    # Basic arithmetic\n",
      "    avg = sum(numbers) / len(numbers)\n",
      "\n",
      "    print(greet(\"Python\"))\n",
      "    print(\"Evens:\", evens)\n",
      "    print(\"Squares:\", squares)\n",
      "    print(\"Average:\", round(avg, 2))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "When you run the second example, you’ll see output like:\n",
      "Hello, Python!\n",
      "Evens: [2, 4]\n",
      "Squares: {1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n",
      "Average: 3.0\n",
      "\n",
      "Would you like an example in a specific area (e.g., web, data analysis, automation)?\n"
     ]
    }
   ],
   "source": [
    "# Add to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response1.choices[0].message.content})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you show me a simple example?\"})\n",
    "\n",
    "# Second exchange (with context)\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=conversation\n",
    ")\n",
    "\n",
    "print(\"\\nSecond response (with context):\")\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety Filters\n",
    "\n",
    "The safety filters are not a part of the model itself but something that is added on top (for example by OpenAI)\n",
    "\n",
    "<img src=\"./data/guardrails.png\" width=\"700\" height=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering {#prompt-engineering}\n",
    "\n",
    "Prompt engineering is the art of crafting effective prompts to get the best results from LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System and User Prompts\n",
    "\n",
    "#### System Prompts\n",
    "-  System prompts set the behavior and personality of the assistant:` \"role\": \"system\"`\n",
    "-  User prompts are for the prompt of the user `\"role\": \"user`\n",
    "-  Assistant: replies from the LLM itself are tagged as \"assistant\" `\"role\": \"assistant\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With system prompt:\n",
      "Norsk: Hovedstaden i Norge er Oslo.\n",
      "English: The capital of Norway is Oslo.\n"
     ]
    }
   ],
   "source": [
    "# Example with system prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful Norwegian language tutor. Always provide answers in both Norwegian and English.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Norway?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"With system prompt:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Shot / Few-Shot Prompting\n",
    "\n",
    "Provide examples to teach the model the desired format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classification:\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# Few-shot learning example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Classify the sentiment of the given text as positive, negative, or neutral.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I love this product!\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"positive\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"This is terrible.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"negative\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The weather is okay today.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Sentiment classification:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT) and Step -by-Step Prompting\n",
    "\n",
    "One of the most powerful techniques for improving the reasoning capabilities of language models is to explicitly request the chain-of-thought or step-by-step reasoning. \n",
    "\n",
    "### What is Chain-of-Thought Prompting?\n",
    "\n",
    "In chain-of-thought prompting, you instruct the model to generate a series of intermediate reasoning steps that connect the question to the answer. For instance, rather than issuing a prompt like:\n",
    "\n",
    "> “What is 15% of 200?”\n",
    "\n",
    "you might write:\n",
    "\n",
    "> “Calculate 15% of 200. First, write down each step of your reasoning in detail, then provide the final answer.”\n",
    "\n",
    "This might yield a response like:\n",
    "\n",
    "#### Reasoning:\n",
    "\n",
    "1. 15% as a decimal is 0.15.\n",
    "2. Multiply 0.15 by 200 to find 15% of 200.\n",
    "3. \\(0.15 \\times 200 = 30\\).\n",
    "\n",
    "**Answer:** 30\n",
    "\n",
    "### Benefits of Step-by-Step Reasoning\n",
    "\n",
    "- **Improved Accuracy:** Explicitly breaking down the reasoning often leads to fewer errors. The model “forces” itself to check each step logically.\n",
    "  \n",
    "- **Transparency:** You can inspect each step to verify correctness. If something goes wrong, you can identify the error more easily.\n",
    "\n",
    "- **Error Correction:** If the model’s chain-of-thought is partially incorrect, you can prompt it to reconsider or correct specific steps, rather than having to re-ask the entire question. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT example:\n",
      "- Convert 15% to a decimal: 15% = 15/100 = 0.15\n",
      "- Multiply by 200: 0.15 × 200 = 30\n",
      "\n",
      "Answer: 30\n"
     ]
    }
   ],
   "source": [
    "# CoT example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 15% of 200? Please explain your reasoning step-by-step.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"CoT example:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structured Output {#structured-output}\n",
    "\n",
    "Text-based answers are hard to process further. With structured output we can get a consistent, structured response from the LLM using JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple version (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output:\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"topics\": [\"website design\", \"user interface\", \"loading speed\", \"customization options\"],\n",
      "  \"summary\": \"The user praises the website's new design, intuitive interface, and fast loading speed, but wishes for more customization options.\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "✅ Successfully parsed as JSON:\n",
      "  sentiment: positive\n",
      "  topics: ['website design', 'user interface', 'loading speed', 'customization options']\n",
      "  summary: The user praises the website's new design, intuitive interface, and fast loading speed, but wishes for more customization options.\n",
      "  confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Structured output example\n",
    "structured_prompt = \"\"\"\n",
    "Analyze the following text and return a JSON response with the following structure:\n",
    "{\n",
    "    \"sentiment\": \"positive/negative/neutral\",\n",
    "    \"topics\": [\"list\", \"of\", \"main\", \"topics\"],\n",
    "    \"summary\": \"brief summary\",\n",
    "    \"confidence\": 0.95\n",
    "}\n",
    "\n",
    "Text to analyze: \"I absolutely love the new design of this website! \n",
    "The user interface is intuitive and the loading speed is impressive. \n",
    "However, I wish there were more customization options available.\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}]\n",
    ")\n",
    "\n",
    "print(\"Structured output:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Try to parse as JSON\n",
    "try:\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    print(\"\\n✅ Successfully parsed as JSON:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\n❌ Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with chat.completions.parse and pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"events\":[{\"name\":\"Napoleonic Wars\",\"date\":\"1803-1815\",\"participants\":[\"France\",\"United Kingdom\",\"Prussia\",\"Russia\",\"Austria\",\"Spain\",\"Portugal\",\"Sweden\",\"Ottoman Empire\"]},{\"name\":\"Congress of Vienna\",\"date\":\"1814-1815\",\"participants\":[\"United Kingdom\",\"Austria\",\"Prussia\",\"Russia\",\"France\"]},{\"name\":\"American Civil War\",\"date\":\"1861-1865\",\"participants\":[\"Union (United States)\",\"Confederate States\"]},{\"name\":\"Industrial Revolution\",\"date\":\"late 18th century - 19th century\",\"participants\":[\"United Kingdom\",\"United States\",\"Western Europe\"]},{\"name\":\"Unification of Germany\",\"date\":\"1871\",\"participants\":[\"Prussia\",\"German States\"]}]}\n",
      "\n",
      "✅ Successfully validated with Pydantic:\n",
      "  - Napoleonic Wars on 1803-1815 with participants: France, United Kingdom, Prussia, Russia, Austria, Spain, Portugal, Sweden, Ottoman Empire\n",
      "  - Congress of Vienna on 1814-1815 with participants: United Kingdom, Austria, Prussia, Russia, France\n",
      "  - American Civil War on 1861-1865 with participants: Union (United States), Confederate States\n",
      "  - Industrial Revolution on late 18th century - 19th century with participants: United Kingdom, United States, Western Europe\n",
      "  - Unification of Germany on 1871 with participants: Prussia, German States\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"List 5 important events in the XIX century\"}]\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "  name: str\n",
    "  date: str\n",
    "  participants: list[str]\n",
    "\n",
    "class EventsList(BaseModel):\n",
    "    events: list[CalendarEvent]\n",
    "\n",
    "resp = client.chat.completions.parse(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=messages,\n",
    "    response_format=EventsList\n",
    ")\n",
    "\n",
    "\n",
    "print(resp.choices[0].message.content)\n",
    "\n",
    "# Verify with pydantic\n",
    "try:\n",
    "    events_list = EventsList.model_validate_json(resp.choices[0].message.content)\n",
    "    print(\"\\n✅ Successfully validated with Pydantic:\")\n",
    "    for event in events_list.events:\n",
    "        print(f\"  - {event.name} on {event.date} with participants: {', '.join(event.participants)}\")   \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Pydantic validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Function Calling {#function-calling}\n",
    "\n",
    "Function calling allows LLMs to interact with external tools and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that the model can call\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get current weather for a location.\"\"\"\n",
    "    # This would typically call a real weather API\n",
    "    weather_data = {\n",
    "        \"oslo\": \"15°C, partly cloudy\",\n",
    "        \"bergen\": \"12°C, rainy\",\n",
    "        \"trondheim\": \"10°C, sunny\"\n",
    "    }\n",
    "    return weather_data.get(location.lower(), \"Weather data not available\")\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Only allow basic math operations for safety\n",
    "        allowed_chars = set('0123456789+-*/.() ')\n",
    "        if all(c in allowed_chars for c in expression):\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        else:\n",
    "            return \"Invalid expression\"\n",
    "    except:\n",
    "        return \"Error in calculation\"\n",
    "\n",
    "# Function definitions for the API\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather information for a specific location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name to get weather for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform mathematical calculations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response:\n",
      "Model wants to call functions: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/0c65z5vs2393gm6gsskc5ndh0000gp/T/ipykernel_22014/3382174086.py:48: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.choices[0].message.dict(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final response:\n",
      "- Oslo weather: 15°C, partly cloudy\n",
      "- 15 * 7 + 23 = 128\n"
     ]
    }
   ],
   "source": [
    "# Function calling example\n",
    "def handle_function_call(response):\n",
    "    \"\"\"Handle function calls from the model.\"\"\"\n",
    "    function_map = {\n",
    "        \"get_weather\": get_weather,\n",
    "        \"calculate\": calculate\n",
    "    }\n",
    "    \n",
    "    message = response.choices[0].message\n",
    "    \n",
    "    if message.tool_calls:\n",
    "        results = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name in function_map:\n",
    "                result = function_map[function_name](**function_args)\n",
    "                results.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": result\n",
    "                })\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "# Test function calling\n",
    "user_message = \"What's the weather like in Oslo? Also, what is 15 * 7 + 23?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure/gpt-5\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    tools=[{\"type\": \"function\", \"function\": func} for func in functions],\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Initial response:\")\n",
    "print(f\"Model wants to call functions: {bool(response.choices[0].message.tool_calls)}\")\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    # Execute the function calls\n",
    "    function_results = handle_function_call(response)\n",
    "    \n",
    "    # Send the results back to get the final answer\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        response.choices[0].message.dict(),\n",
    "    ] + function_results\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-5\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal response:\")\n",
    "    print(final_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"\\nDirect response:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Output {#streaming}\n",
    "\n",
    "Streaming allows you to receive partial responses as they're generated, providing a better user experience for longer responses.\n",
    "\n",
    "The simplest way to use streaming is with `stream=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Streaming response:\n",
      "--------------------------------------------------\n",
      "Machine learning (ML) is a subset of artificial intelligence (AI) that involves the development of algorithms that enable computers to learn and make decisions or predictions based on data, without being explicitly programmed for the specific task. It revolves around the idea of constructing algorithms that can identify patterns within data and utilize these patterns to perform tasks or make decisions.\n",
      "\n",
      "### How Machine Learning Works\n",
      "\n",
      "1. **Data Collection:** Machine learning begins with collecting data relevant to the task. This data can be in various forms, such as structured data (like spreadsheets) or unstructured data (like text, images, or videos).\n",
      "\n",
      "2. **Data Preprocessing:** Raw data often needs to be cleaned and structured before feeding it into a machine learning model. This process involves handling missing values, normalizing data, and transforming data into a format suitable for analysis.\n",
      "\n",
      "3. **Choosing a Model:** The next step involves selecting the appropriate machine learning model or algorithm. This choice depends on the nature of the problem, the type of data, and the specific goals of the task.\n",
      "\n",
      "4. **Training the Model:** During the training phase, the model learns by processing the training data, identifying patterns, and adjusting parameters to improve accuracy. The model is fed input data and corresponding output labels, allowing it to learn the relationships between them.\n",
      "\n",
      "5. **Evaluation:** After training, the model is evaluated using a separate set of data (testing set) to ensure that it generalizes well to new, unseen data. Evaluation metrics like accuracy, precision, recall, and F1-score help assess the model's performance.\n",
      "\n",
      "6. **Hyperparameter Tuning:** Often, models have hyperparameters—parameters not directly learned from the training data. Tuning these hyperparameters can greatly affect the model's performance, involving strategies like grid search and randomized search.\n",
      "\n",
      "7. **Deployment:** Once a model is deemed satisfactory, it can be deployed into the real world. It will then process new inputs and make predictions or recommendations. Continuous monitoring is necessary to ensure that it performs well over time.\n",
      "\n",
      "### Main Types of Machine Learning\n",
      "\n",
      "1. **Supervised Learning:** In supervised learning, the model is trained on labeled data. It learns the mapping between input features and output labels. This type of learning is useful for tasks like classification (e.g., spam detection) and regression (e.g., predicting prices).\n",
      "\n",
      "2. **Unsupervised Learning:** Unlike supervised learning, unsupervised learning deals with unlabeled data. The model identifies patterns and structures within the\n",
      "--------------------------------------------------\n",
      "✅ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Basic streaming example\n",
    "import time\n",
    "\n",
    "prompt = \"Write a detailed explanation of how machine learning works, including the main types and applications.\"\n",
    "\n",
    "print(\"🔄 Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"azure/gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Collect and display chunks as they arrive\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        chunk_content = chunk.choices[0].delta.content\n",
    "        full_response += chunk_content\n",
    "        print(chunk_content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"✅ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reasoning Models {#reasoning-models}\n",
    "\n",
    "Different models have different capabilities for complex reasoning tasks.\n",
    "\n",
    "### How reasoning works\n",
    "Reasoning models introduce reasoning tokens in addition to input and output tokens. The models use these reasoning tokens to \"think,\" breaking down the prompt and considering multiple approaches to generating a response. After generating reasoning tokens, the model produces an answer as visible completion tokens and discards the reasoning tokens from its context.\n",
    "\n",
    "- Users control the depth of this internal reasoning process with the reasoning_effort parameter (e.g., \"low,\" \"medium,\" or \"high\"), which influences the number of reasoning tokens generated to balance speed and accuracy.\n",
    "\n",
    "- A reasoning model uses internal, invisible \"reasoning tokens\" to break down complex prompts and plan multi-step tasks before generating a final, visible answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if a model supports reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'litellm' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(litellm.supports_reasoning(model=\u001b[33m\"\u001b[39m\u001b[33mvertex_ai/claude-sonnet-4\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(litellm.supports_reasoning(model=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/litellm/__init__.py:1288\u001b[39m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01movhcloud\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membedding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OVHCloudEmbeddingConfig\n\u001b[32m   1287\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlemonade\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LemonadeChatConfig\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1289\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcustom_httpx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_client_cleanup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m close_litellm_async_clients\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/litellm/main.py:150\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mazure_ai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureAIEmbedding\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbedrock\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockConverseLLM, BedrockLLM\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbedrock\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockEmbedding\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbedrock\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_handler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockImageGeneration\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbytez\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytezChatConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/litellm/llms/bedrock/embed/embedding.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BEDROCK_EMBEDDING_PROVIDERS_LITERAL\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcohere\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membed\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m embedding \u001b[38;5;28;01mas\u001b[39;00m cohere_embedding\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcustom_httpx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhttp_handler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     AsyncHTTPHandler,\n\u001b[32m     17\u001b[39m     HTTPHandler,\n\u001b[32m     18\u001b[39m     _get_httpx_client,\n\u001b[32m     19\u001b[39m     get_async_httpx_client,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msecret_managers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_secret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Prosjekter/kihub-llm-workshop/llm-test/lib/python3.11/site-packages/litellm/llms/cohere/embed/handler.py:53\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mself\u001b[39m.response = httpx.Response(status_code=status_code, request=\u001b[38;5;28mself\u001b[39m.request)\n\u001b[32m     44\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m             \u001b[38;5;28mself\u001b[39m.message\n\u001b[32m     46\u001b[39m         )  \u001b[38;5;66;03m# Call the base class constructor with the parameters it needs\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masync_embedding\u001b[39m(\n\u001b[32m     50\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     51\u001b[39m     data: Union[\u001b[38;5;28mdict\u001b[39m, CohereEmbeddingRequest],\n\u001b[32m     52\u001b[39m     \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mlist\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     model_response: \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m.EmbeddingResponse,\n\u001b[32m     54\u001b[39m     timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, httpx.Timeout]],\n\u001b[32m     55\u001b[39m     logging_obj: LiteLLMLoggingObj,\n\u001b[32m     56\u001b[39m     optional_params: \u001b[38;5;28mdict\u001b[39m,\n\u001b[32m     57\u001b[39m     api_base: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     58\u001b[39m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m     59\u001b[39m     headers: \u001b[38;5;28mdict\u001b[39m,\n\u001b[32m     60\u001b[39m     encoding: Callable,\n\u001b[32m     61\u001b[39m     client: Optional[AsyncHTTPHandler] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     62\u001b[39m ):\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[32m     64\u001b[39m     logging_obj.pre_call(\n\u001b[32m     65\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m     66\u001b[39m         api_key=api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m         },\n\u001b[32m     72\u001b[39m     )\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m## COMPLETION CALL\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'litellm' has no attribute 'utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "print(litellm.supports_reasoning(model=\"vertex_ai/claude-sonnet-4\"))\n",
    "print(litellm.supports_reasoning(model=\"openai/gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning model response:\n",
      "ChatCompletion(id='chatcmpl-dfd70e90-44f8-461e-bfba-169909b11fc4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=\"This is a straightforward factual question. The capital of France is Paris. This is basic geography knowledge that I'm confident about.\", thinking_blocks=[{'type': 'thinking', 'thinking': \"This is a straightforward factual question. The capital of France is Paris. This is basic geography knowledge that I'm confident about.\", 'signature': 'ErICCkgICBACGAIqQCkH/2/r5RnhmRPpeQJHPH4xu3wLTFTJ6Irb+MOenCiTfRIN2Biyn2RaEusPU0lyhAsv8KdupmrUE3Eya6WjK/8SDIktmkuiZkl3SX7sehoMry91jHNXxJTgGpNYIjBmi5261dWtfAl8rm5ZyzowKGH/naSx5UkwNYjayrXlbQVZteBdHWbSxlB673WlArUqlwExwLe3q/k+1JqwDo3ixqhyv5+YP91L1//kdzwLT+A23/vFwzB7VWkz6gCDulM3Vf96QPA2sXGyXurkjjkChtlJ/9UYY7e0L99EpVU2Z8v8t2uZnPxLuXpBbaMPP9BCH91SrfGo6YFwJzBJM8NLO07OTNHB3MbhDRgqZpcT1gpUHmpOnKvASCwW8SHmqnS/dvRZ0JVG9hOeGAE='}]))], created=1759928887, model='claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=42, total_tokens=88, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=25, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "The capital of France is Paris.\n",
      "This is a straightforward factual question. The capital of France is Paris. This is basic geography knowledge that I'm confident about.\n",
      "[{'type': 'thinking', 'thinking': \"This is a straightforward factual question. The capital of France is Paris. This is basic geography knowledge that I'm confident about.\", 'signature': 'ErICCkgICBACGAIqQCkH/2/r5RnhmRPpeQJHPH4xu3wLTFTJ6Irb+MOenCiTfRIN2Biyn2RaEusPU0lyhAsv8KdupmrUE3Eya6WjK/8SDIktmkuiZkl3SX7sehoMry91jHNXxJTgGpNYIjBmi5261dWtfAl8rm5ZyzowKGH/naSx5UkwNYjayrXlbQVZteBdHWbSxlB673WlArUqlwExwLe3q/k+1JqwDo3ixqhyv5+YP91L1//kdzwLT+A23/vFwzB7VWkz6gCDulM3Vf96QPA2sXGyXurkjjkChtlJ/9UYY7e0L99EpVU2Z8v8t2uZnPxLuXpBbaMPP9BCH91SrfGo6YFwJzBJM8NLO07OTNHB3MbhDRgqZpcT1gpUHmpOnKvASCwW8SHmqnS/dvRZ0JVG9hOeGAE='}]\n"
     ]
    }
   ],
   "source": [
    "# Example of reasoning models - some models support reasoning_effort parameter\n",
    "response = client.chat.completions.create(\n",
    "    model=\"vertex_ai/claude-sonnet-4\",  # Try reasoning model if available\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    reasoning_effort=\"low\",  \n",
    ")\n",
    "print(\"Reasoning model response:\")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.reasoning_content)\n",
    "print(response.choices[0].message.thinking_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning model response:\n",
      "ChatCompletion(id='chatcmpl-b4ae8118-7cfb-401b-b4da-6d3a6840ce80', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To find the average speed for the entire journey, I need to calculate the total distance and total time.\\n\\n**Given information:**\\n- First segment: 120 km in 1.5 hours\\n- Second segment: 180 km in 2 hours\\n\\n**Calculations:**\\n- Total distance = 120 km + 180 km = 300 km\\n- Total time = 1.5 hours + 2 hours = 3.5 hours\\n\\n**Average speed formula:**\\nAverage speed = Total distance ÷ Total time\\n\\nAverage speed = 300 km ÷ 3.5 hours = **85.7 km/h**\\n\\nTherefore, the average speed for the entire journey is 85.7 km/h (or 600/7 km/h if you want the exact fraction).', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content='To find the average speed for the entire journey, I need to calculate the total distance traveled and the total time taken, then use the formula:\\n\\nAverage speed = Total distance / Total time\\n\\nLet me break this down:\\n\\nFirst part of journey:\\n- Distance = 120 km\\n- Time = 1.5 hours\\n\\nSecond part of journey:\\n- Distance = 180 km\\n- Time = 2 hours\\n\\nTotal distance = 120 km + 180 km = 300 km\\nTotal time = 1.5 hours + 2 hours = 3.5 hours\\n\\nAverage speed = 300 km / 3.5 hours = 300/3.5 = 85.71... km/h\\n\\nLet me double-check this calculation:\\n300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h', thinking_blocks=[{'type': 'thinking', 'thinking': 'To find the average speed for the entire journey, I need to calculate the total distance traveled and the total time taken, then use the formula:\\n\\nAverage speed = Total distance / Total time\\n\\nLet me break this down:\\n\\nFirst part of journey:\\n- Distance = 120 km\\n- Time = 1.5 hours\\n\\nSecond part of journey:\\n- Distance = 180 km\\n- Time = 2 hours\\n\\nTotal distance = 120 km + 180 km = 300 km\\nTotal time = 1.5 hours + 2 hours = 3.5 hours\\n\\nAverage speed = 300 km / 3.5 hours = 300/3.5 = 85.71... km/h\\n\\nLet me double-check this calculation:\\n300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h', 'signature': 'EvwFCkgICBACGAIqQLI7xxOC7FspyGq/RlQSB5r6Mp9xa1JaLAczZ3V66y0N6kAXPWAn7U9s0N18dWLYIMu181WbSljuR4alH64Ki2MSDMQZOesnHxKo4mvsbxoMWF/szRIlOPCfMxHMIjC4F2Bk3WBry9BhTqdsx2Ylvb+JB1Lb9TUc+TswI/hjVgKa/V+WrKhwOcHuWelnceUq4QTvYB5cV9gA77/fCnp+5AQGkpl/bkFKVObnHzkHpMF+lzylo1X8diIfxrNhoocCL4vFrwAt//LmjmC40VYVZTa8JNWCJx2MJp7dLhXEcUNUjVdPlAYOJ9WfF9cCpXSL+2gqEeGXPzjoWvF6M3b1/UKHt5/8Qw0Z8PvwSVO1eirJi1DeQKYrRnpjvrvU+PM8T5s8aAuFoW+4Dwoa7oqJxKqMGOvVQ4LxmOd9NSnHlk6SpVed/wCUIy7nh1HU8xolEkRTxKahEF/mlkxMIuqR4frxcU0jbCqxArKAZJ1DXNaK+1/zfzbdqPyPEEpZ4vDunUkXETjg/dZ/ySRPiNZV5NxXfBmy0rPCA3V115Xs1YJKONIQw00/q4QM5212ABeY2I0t+nN0cO0dea4ErcZZhTzq4zEeY05gXgD9TIu3NUVTeOPxrJZuanD4MNf49GDnkfZe6xCwZHmkiHjDo3mfDdVFpntSRhvnOaKd9uQ7cOvJPgy3zRwnfUOZ8Y5OIiAYtG4L5rIXu60wAQXHgj7E6I8AxkIck7UsS/ClOAknt6JtY8uc09svlfZgqz9/ih3GKx9vkW5aa+Utx2ZwqhJ1VeVFEAwAdkvOE7jGxtEzoMyjTL0b7T84THtf6knpldDYBbHMXiXSg0bjJqVDZ4XfM18EEr9X6nMQ4PKZL5cF7z4YPXx4rbUUTgJAOGqAkIc2i0QS1W2LsYV5ROo3P1KKb/81WEMHjJIfKiGjEvbNXcFFOD1O5ilf1qdeBIaoI97kQR4dvbQ1J4e7S3HxVjLEBTIrjzV+JD9EeppeBdxm8D40jJsYAQ=='}]))], created=1760449259, model='claude-sonnet-4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=422, prompt_tokens=75, total_tokens=497, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=192, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "🔄  Output: To find the average speed for the entire journey, I need to calculate the total distance and total time.\n",
      "\n",
      "**Given information:**\n",
      "- First segment: 120 km in 1.5 hours\n",
      "- Second segment: 180 km in 2 hours\n",
      "\n",
      "**Calculations:**\n",
      "- Total distance = 120 km + 180 km = 300 km\n",
      "- Total time = 1.5 hours + 2 hours = 3.5 hours\n",
      "\n",
      "**Average speed formula:**\n",
      "Average speed = Total distance ÷ Total time\n",
      "\n",
      "Average speed = 300 km ÷ 3.5 hours = **85.7 km/h**\n",
      "\n",
      "Therefore, the average speed for the entire journey is 85.7 km/h (or 600/7 km/h if you want the exact fraction).\n",
      "🔄  Reasoning Content: To find the average speed for the entire journey, I need to calculate the total distance traveled and the total time taken, then use the formula:\n",
      "\n",
      "Average speed = Total distance / Total time\n",
      "\n",
      "Let me break this down:\n",
      "\n",
      "First part of journey:\n",
      "- Distance = 120 km\n",
      "- Time = 1.5 hours\n",
      "\n",
      "Second part of journey:\n",
      "- Distance = 180 km\n",
      "- Time = 2 hours\n",
      "\n",
      "Total distance = 120 km + 180 km = 300 km\n",
      "Total time = 1.5 hours + 2 hours = 3.5 hours\n",
      "\n",
      "Average speed = 300 km / 3.5 hours = 300/3.5 = 85.71... km/h\n",
      "\n",
      "Let me double-check this calculation:\n",
      "300 ÷ 3.5 = 300 ÷ (7/2) = 300 × (2/7) = 600/7 ≈ 85.71 km/h\n"
     ]
    }
   ],
   "source": [
    "# Example of reasoning models - some models support reasoning_effort parameter\n",
    "response = client.chat.completions.create(\n",
    "    model=\"vertex_ai/claude-sonnet-4\",  # Try reasoning model if available\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"If a train travels 120 km in 1.5 hours, and then travels another 180 km in 2 hours, what is the average speed for the entire journey??\"},\n",
    "    ],\n",
    "    reasoning_effort=\"high\",  \n",
    ")\n",
    "print(\"Reasoning model response:\")\n",
    "print(response)\n",
    "print(f'🔄  Output: '+response.choices[0].message.content)\n",
    "print(f'🔄  Reasoning Content: '+response.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 A little advice on prompting\n",
    "\n",
    "There are some differences to consider when prompting a reasoning model. \n",
    "\n",
    "- Reasoning models provide better results on tasks with only high-level guidance, while GPT models often benefit from very precise instructions.\n",
    "-  A reasoning model is like a senior co-worker—you can give them a goal to achieve and trust them to work out the details.\n",
    "- A GPT model is like a junior coworker—they'll perform best with explicit instructions to create a specific output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG - Chat with Your Data {#rag}\n",
    "\n",
    "Retrieval Augmented Generation allows LLMs to access and reason over external knowledge.\n",
    "\n",
    "It consists of two steps: \n",
    "- retrieval (mostly embedding-based) and \n",
    "- augmented generation using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❓ Question: What is the capital of Norway?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. The capital of Norway is Oslo.\n",
      "  3. Norway has a population of approximately 5.4 million people.\n",
      "🤖 Answer: Oslo\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: Tell me about Norwegian languages\n",
      "📚 Retrieved docs: 3\n",
      "  1. The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\n",
      "  2. Norway is not a member of the European Union but is part of the EEA.\n",
      "  3. Machine learning is a subset of artificial intelligence.\n",
      "🤖 Answer: According to the provided context, Norway’s official language is Norwegian, which has two written forms: Bokmål and Nynorsk. The context doesn’t provide further details about other languages or features.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: What is Python programming language?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. The capital of Norway is Oslo.\n",
      "  3. The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\n",
      "🤖 Answer: The context doesn’t contain enough information to answer what the Python programming language is.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "❓ Question: How many people live in Sweden?\n",
      "📚 Retrieved docs: 3\n",
      "  1. Norway is a Scandinavian country in Northern Europe.\n",
      "  2. Norway has a population of approximately 5.4 million people.\n",
      "  3. The country has significant oil and gas reserves in the North Sea.\n",
      "🤖 Answer: The context doesn’t provide information about Sweden’s population, so I can’t answer based on the provided context.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# Simple RAG example with in-memory knowledge base\n",
    "knowledge_base = {\n",
    "    \"norway_facts\": [\n",
    "        \"Norway is a Scandinavian country in Northern Europe.\",\n",
    "        \"The capital of Norway is Oslo.\",\n",
    "        \"Norway has a population of approximately 5.4 million people.\",\n",
    "        \"The official language is Norwegian, with two written forms: Bokmål and Nynorsk.\",\n",
    "        \"Norway is famous for its fjords, northern lights, and midnight sun.\",\n",
    "        \"The country has significant oil and gas reserves in the North Sea.\",\n",
    "        \"Norway is not a member of the European Union but is part of the EEA.\"\n",
    "    ],\n",
    "    \"tech_facts\": [\n",
    "        \"Python is a high-level programming language created by Guido van Rossum.\",\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"APIs (Application Programming Interfaces) allow different software systems to communicate.\",\n",
    "        \"Cloud computing provides on-demand access to computing resources.\",\n",
    "        \"Git is a distributed version control system.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def simple_retrieval(query: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for category, docs in knowledge_base.items():\n",
    "        for doc in docs:\n",
    "            # Simple keyword matching\n",
    "            if any(word in doc.lower() for word in query_lower.split()):\n",
    "                relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs[:k]\n",
    "\n",
    "def rag_query(user_question: str) -> str:\n",
    "    \"\"\"Perform RAG: retrieve relevant docs and generate answer.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    relevant_docs = simple_retrieval(user_question)\n",
    "    \n",
    "    # Step 2: Create context from retrieved documents\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc in relevant_docs])\n",
    "    \n",
    "    # Step 3: Generate answer using context\n",
    "    rag_prompt = f\"\"\"\n",
    "    Answer the following question using the provided context. If the context doesn't contain \n",
    "    enough information to answer the question, say so.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {user_question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"azure/gpt-5\",\n",
    "        messages=[{\"role\": \"user\", \"content\": rag_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content, relevant_docs\n",
    "\n",
    "# Test RAG system\n",
    "questions = [\n",
    "    \"What is the capital of Norway?\",\n",
    "    \"Tell me about Norwegian languages\",\n",
    "    \"What is Python programming language?\",\n",
    "    \"How many people live in Sweden?\"  # This shows limited knowledge\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    answer, docs = rag_query(question)\n",
    "    print(f\"📚 Retrieved docs: {len(docs)}\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"  {i}. {doc}\")\n",
    "    print(f\"🤖 Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector-based RAG (Conceptual)\n",
    "\n",
    "In practice, RAG systems use vector embeddings for more sophisticated retrieval:\n",
    "\n",
    "Advanced RAG process: \n",
    "\n",
    "1. Document Processing:\n",
    "   - Split documents into chunks\n",
    "   - Generate embeddings for each chunk\n",
    "   - Store in vector database (e.g., Pinecone, Weaviate, ChromaDB)\n",
    "\n",
    "2. Query Processing:\n",
    "   - Generate embedding for user question\n",
    "   - Find similar document chunks using cosine similarity\n",
    "   - Retrieve top-k most relevant chunks\n",
    "\n",
    "3. Generation:\n",
    "   - Combine retrieved chunks into context\n",
    "   - Generate answer using LLM + context\n",
    "   - Optionally include source citations\n",
    "\n",
    "🔧 Tools for Production RAG:\n",
    "- LangChain / LlamaIndex for orchestration\n",
    "- OpenAI/Cohere embeddings for vectors\n",
    "- Vector databases for storage\n",
    "- Chunking strategies for optimal retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3 Langchain example \n",
    "# Taken from https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-S2f4kVB6wznto-kfDPqfuw\"\n",
    "\n",
    "\n",
    "# Assuming your LiteLLM Proxy is running on localhost:4000\n",
    "llm = ChatOpenAI(\n",
    "    model=\"azure/gpt-4o\", # or any model configured in your LiteLLM Proxy\n",
    "    temperature=0,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",  base_url=base_url)\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate]) §We'll use LangGraph to tie together the retrieval and generation steps into a single application\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable sub-tasks or steps. It can be achieved through techniques like simple prompting, task-specific instructions, or human inputs. This approach helps in enhancing model performance by allowing it to handle complex tasks more effectively through step-by-step reasoning.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know who Paul is based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who is Paul?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agents {#agents}\n",
    "\n",
    "AI agents can make decisions, use tools, and take actions to accomplish goals with minimal human intervention. \n",
    "\n",
    "Often, specialized agents for different tasks interact together in a multi-agent system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 User Goal: I need to know the weather in Oslo and calculate what 25% of 240 is\n",
      "============================================================\n",
      "🎯 Plan created with 3 steps\n",
      "\n",
      "🔄 Step 1: Retrieve current weather for Oslo\n",
      "  🔧 Used tool 'get_weather': 15°C, partly cloudy\n",
      "\n",
      "🔄 Step 2: Calculate 25% of 240\n",
      "  🔧 Used tool 'calculate': Calculation result: 60.0\n",
      "\n",
      "🔄 Step 3: Present the weather and calculation results to the user\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "✅ Final Result:\n",
      "Here’s the information you asked for:\n",
      "\n",
      "- Weather in Oslo: 15°C, partly cloudy\n",
      "- 25% of 240: 60.0\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "🎯 User Goal: Find information about Norway's population and calculate how many people that would be per square kilometer if Norway is 385,207 km²\n",
      "============================================================\n",
      "🎯 Plan created with 4 steps\n",
      "\n",
      "🔄 Step 1: Search for the most recent official population of Norway.\n",
      "  🔧 Used tool 'search_knowledge': Found information: Norway is a Scandinavian country in Northern Europe. The capital of Norway is Oslo.\n",
      "\n",
      "🔄 Step 2: Extract the population number and its reference year from the search results.\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "🔄 Step 3: Compute people per square kilometer by dividing the population by 385,207.\n",
      "  🔧 Used tool 'calculate': Invalid mathematical expression\n",
      "\n",
      "🔄 Step 4: Present the calculated density with units (people/km²) and cite the population source and year.\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "✅ Final Result:\n",
      "- Population and year: Not found in the provided search results. The results only state that Norway is a Scandinavian country and that Oslo is the capital; no population figure or reference year was provided, so I cannot extract them.\n",
      "- Note on calculation: A previous attempt reported an “Invalid mathematical expression.” The correct calculation is:\n",
      "  population density (people/km²) = population / 385,207\n",
      "- Next step: If you provide Norway’s population and its reference year (or allow me to look it up from a reliable source such as Statistics Norway or the World Bank), I will compute the density in people/km² and cite the source and year.\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "🎯 User Goal: Tell me about Python programming and calculate how many days are in 5 years\n",
      "============================================================\n",
      "🎯 Plan created with 9 steps\n",
      "\n",
      "🔄 Step 1: Ask the user about their experience level and what aspects of Python they want covered (basics, syntax, data structures, OOP, libraries, tooling, use cases), plus preferred depth and format.\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "🔄 Step 2: Search the knowledge base for an up-to-date overview of Python programming, including what it is, features, strengths, common use cases, and learning resources.\n",
      "  🔧 Used tool 'search_knowledge': Found information: Python is a high-level programming language created by Guido van Rossum. Machine learning is a subset of artificial intelligence.\n",
      "\n",
      "🔄 Step 3: Search the knowledge base for the latest stable Python version and official documentation link.\n",
      "  🔧 Used tool 'search_knowledge': Found information: The official language is Norwegian, with two written forms: Bokmål and Nynorsk. Python is a high-level programming language created by Guido van Rossum.\n",
      "\n",
      "🔄 Step 4: Synthesize and present a concise, structured explanation of Python programming tailored to the user's preferences, referencing the official docs and key resources.\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "🔄 Step 5: Ask the user whether to calculate days in 5 years assuming 365 days/year, account for leap years, or use a specific 5-year date range.\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "🔄 Step 6: If assuming standard 365-day years, compute the total days as 5 * 365 and report the result.\n",
      "  🔧 Used tool 'calculate': Calculation result: 1825\n",
      "\n",
      "🔄 Step 7: If accounting for a typical case with one leap day within 5 years (without a specific date range), compute 5 * 365 + 1 and report the result.\n",
      "  🔧 Used tool 'calculate': Calculation result: 1826\n",
      "\n",
      "🔄 Step 8: If the user provides a specific 5-year date range, determine how many leap years occur in that span using leap-year rules (divisible by 4, except centuries not divisible by 400).\n",
      "  ℹ️  Information step (no tool required)\n",
      "\n",
      "🔄 Step 9: Compute the exact total days for the specified span as 5 * 365 + number_of_leap_years and report the result.\n",
      "  🔧 Used tool 'calculate': Invalid mathematical expression\n",
      "\n",
      "✅ Final Result:\n",
      "Here’s a quick, structured primer on Python plus the 5-year day counts. Tell me your preferences and I’ll tailor it further.\n",
      "\n",
      "Python at a glance\n",
      "- What it is: A high-level, interpreted, dynamically typed, general-purpose language created by Guido van Rossum. Emphasizes readability and a vast ecosystem.\n",
      "- Core syntax: Significant indentation; simple control flow (if/for/while); functions (def), modules (import), exceptions (try/except).\n",
      "- Built-in data types: Numbers, str, bool, None; collections like list, tuple, dict, set; powerful comprehensions.\n",
      "- OOP and more: Classes, inheritance, dataclasses, enums; first-class functions and closures; generators and iterators; async/await for concurrency.\n",
      "- Standard library highlights: pathlib, json, datetime, itertools, functools, subprocess, sqlite3—“batteries included.”\n",
      "- Popular libraries:\n",
      "  - Data/science: NumPy, pandas, Matplotlib/Seaborn, SciPy, scikit-learn\n",
      "  - Web: FastAPI, Django, Flask\n",
      "  - ML/AI: PyTorch, TensorFlow\n",
      "  - Automation/HTTP: Requests, Selenium, Playwright\n",
      "- Tooling and best practices: venv/virtualenv, pip (or Poetry); formatting with black and isort; linting with ruff/flake8; testing with pytest; type hints + mypy/pyright; follow PEP 8.\n",
      "- Typical use cases: Scripting/automation, data analysis, web APIs, ML, scientific computing, DevOps tooling, education.\n",
      "- Key resources: Official docs and tutorial (docs.python.org), PEP 8 style guide, PyPI for packages; good community guides like Real Python and “Automate the Boring Stuff.”\n",
      "\n",
      "Days in 5 years\n",
      "- If you assume 365 days per year: 5 × 365 = 1825 days.\n",
      "- If you include a typical one leap day within that span: 1826 days.\n",
      "- Exact count depends on the specific 5-year range: add one day for each leap year in the range (leap years are divisible by 4, except years divisible by 100 unless also divisible by 400).\n",
      "\n",
      "To tailor this for you\n",
      "- What’s your Python experience level (beginner, intermediate, advanced)?\n",
      "- Which topics should I focus on (basics, syntax, data structures, OOP, standard library, libraries for web/data/ML, tooling, best practices, use cases)? Preferred depth and format (bullets, mini-tutorial, code snippets)?\n",
      "- For the 5-year calculation, should I:\n",
      "  - Use 365-day years (1825),\n",
      "  - Assume one leap year (1826), or\n",
      "  - Compute for a specific date range (please share start date; I’ll compute the exact total)?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple agent implementation\n",
    "class SimpleAgent:\n",
    "    def __init__(self, client, model=\"azure/gpt-5\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.conversation_history = []\n",
    "        self.tools = {\n",
    "            \"calculate\": self.calculate,\n",
    "            \"search_knowledge\": self.search_knowledge,\n",
    "            \"get_weather\": self.get_weather\n",
    "        }\n",
    "    \n",
    "    def calculate(self, expression: str) -> str:\n",
    "        \"\"\"Perform mathematical calculations.\"\"\"\n",
    "        try:\n",
    "            allowed_chars = set('0123456789+-*/.() ')\n",
    "            if all(c in allowed_chars for c in expression):\n",
    "                result = eval(expression)\n",
    "                return f\"Calculation result: {result}\"\n",
    "            else:\n",
    "                return \"Invalid mathematical expression\"\n",
    "        except Exception as e:\n",
    "            return f\"Calculation error: {str(e)}\"\n",
    "    \n",
    "    def search_knowledge(self, query: str) -> str:\n",
    "        \"\"\"Search the knowledge base.\"\"\"\n",
    "        docs = simple_retrieval(query, k=2)\n",
    "        if docs:\n",
    "            return f\"Found information: {' '.join(docs)}\"\n",
    "        return \"No relevant information found in knowledge base.\"\n",
    "    \n",
    "    def get_weather(self, location: str) -> str:\n",
    "        \"\"\"Get weather information.\"\"\"\n",
    "        weather_data = {\n",
    "            \"oslo\": \"15°C, partly cloudy\",\n",
    "            \"bergen\": \"12°C, rainy\", \n",
    "            \"trondheim\": \"10°C, sunny\"\n",
    "        }\n",
    "        return weather_data.get(location.lower(), \"Weather data not available for this location\")\n",
    "    \n",
    "    def plan_and_execute(self, user_goal: str) -> str:\n",
    "        \"\"\"Plan steps to achieve user goal and execute them.\"\"\"\n",
    "        \n",
    "        # Step 1: Create a plan\n",
    "        planning_prompt = f\"\"\"\n",
    "        You are an AI agent with access to these tools:\n",
    "        - calculate(expression): Perform mathematical calculations\n",
    "        - search_knowledge(query): Search knowledge base for information\n",
    "        - get_weather(location): Get weather for a location\n",
    "        \n",
    "        User goal: {user_goal}\n",
    "        \n",
    "        Create a step-by-step plan to achieve this goal. For each step, specify:\n",
    "        1. The action to take\n",
    "        2. Which tool to use (if any)\n",
    "        3. What parameters to pass\n",
    "        \n",
    "        Format your response as a JSON list of steps:\n",
    "        [\n",
    "            {{\"step\": 1, \"action\": \"description\", \"tool\": \"tool_name\", \"params\": {{\"param\": \"value\"}}}},\n",
    "            {{\"step\": 2, \"action\": \"description\", \"tool\": null, \"params\": null}}\n",
    "        ]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            plan = json.loads(response.choices[0].message.content)\n",
    "            print(f\"🎯 Plan created with {len(plan)} steps\")\n",
    "            \n",
    "            # Step 2: Execute the plan\n",
    "            results = []\n",
    "            for step in plan:\n",
    "                print(f\"\\n🔄 Step {step['step']}: {step['action']}\")\n",
    "                \n",
    "                if step.get('tool') and step['tool'] in self.tools:\n",
    "                    tool_func = self.tools[step['tool']]\n",
    "                    params = step.get('params', {})\n",
    "                    \n",
    "                    # Execute tool\n",
    "                    if params:\n",
    "                        result = tool_func(**params)\n",
    "                    else:\n",
    "                        result = \"No parameters provided for tool\"\n",
    "                    \n",
    "                    print(f\"  🔧 Used tool '{step['tool']}': {result}\")\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    print(f\"  ℹ️  Information step (no tool required)\")\n",
    "                    results.append(step['action'])\n",
    "            \n",
    "            # Step 3: Synthesize final answer\n",
    "            synthesis_prompt = f\"\"\"\n",
    "            User goal: {user_goal}\n",
    "            \n",
    "            Execution results:\n",
    "            {chr(10).join([f\"- {result}\" for result in results])}\n",
    "            \n",
    "            Provide a final answer to the user that accomplishes their goal based on the execution results.\n",
    "            \"\"\"\n",
    "            \n",
    "            final_response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": synthesis_prompt}]\n",
    "            )\n",
    "            \n",
    "            return final_response.choices[0].message.content\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            return \"Failed to parse execution plan. Please try again.\"\n",
    "        except Exception as e:\n",
    "            return f\"Execution error: {str(e)}\"\n",
    "\n",
    "# Create and test the agent\n",
    "agent = SimpleAgent(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_goals = [\n",
    "    \"I need to know the weather in Oslo and calculate what 25% of 240 is\",\n",
    "    \"Find information about Norway's population and calculate how many people that would be per square kilometer if Norway is 385,207 km²\",\n",
    "    \"Tell me about Python programming and calculate how many days are in 5 years\"\n",
    "]\n",
    "\n",
    "for goal in test_goals:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🎯 User Goal: {goal}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = agent.plan_and_execute(goal)\n",
    "    \n",
    "    print(f\"\\n✅ Final Result:\")\n",
    "    print(result)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Popular Agent Frameworks:\n",
    "\n",
    "- [LangChain/LangGraph](https://www.langchain.com/)\n",
    "- [AutoGen (Microsoft)](https://microsoft.github.io/autogen/)\n",
    "- [AutoGPT](https://agpt.co/)\n",
    "- [CrewAI](https://www.crewai.com/)\n",
    "- [Microsoft Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)\n",
    "- [Llamaindex](https://www.llamaindex.ai/)\n",
    "- [Smolagents](https://huggingface.co/docs/smolagents/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Further reading\n",
    "\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
